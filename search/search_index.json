{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Estimation Alignment Hub","text":"<p>A unified system for improving predictability, reducing delivery risk, and aligning expectations across the full software lifecycle.</p>"},{"location":"#1-why-this-hub-exists","title":"1. Why This Hub Exists","text":"<p>Software teams use multiple estimation methods \u2014 FTEs, story points, sprints, velocity, cost models, forecasting ranges \u2014 but none of them speak the same language. This creates predictable failures:</p> <ul> <li>RFP assumptions don\u2019t match delivery reality</li> <li>discovery/design workload is underestimated</li> <li>story points drift away from original expectations</li> <li>velocity rises while timelines still slip</li> <li>scope expands invisibly until it becomes a crisis</li> <li>teams feel pressure instead of clarity</li> <li>client trust is damaged</li> </ul> <p>This hub introduces a single alignment model for the entire lifecycle by grounding all estimation methods in two shared dimensions:</p> <ul> <li>Complexity (known work)</li> <li>Uncertainty (unknown work)</li> </ul> <p>When these dimensions are explicitly measured and tracked, the organization gains:</p> <ul> <li>predictable delivery</li> <li>early detection of risk</li> <li>better discovery/design planning</li> <li>realistic client expectations</li> <li>clearer trade-off decisions</li> <li>improved collaboration across roles</li> <li>alignment across the 3SF triangle (Engagement \u2194 Delivery \u2194 Value)</li> </ul>"},{"location":"#2-what-this-hub-provides","title":"2. What This Hub Provides","text":"<p>This space contains five tightly connected pages, each answering a different \u201cwhy\u201d and \u201chow\u201d of estimation alignment.</p>"},{"location":"010-quick-start/","title":"Quick Start Guide","text":""},{"location":"010-quick-start/#the-model-complexity-uncertainty-framework","title":"The Model: Complexity &amp; Uncertainty Framework","text":"<p>Why estimation fails, and the shared model that fixes it.</p> <p>This page defines:</p> <ul> <li>the two missing dimensions</li> <li>the lifecycle alignment model</li> <li>complexity &amp; uncertainty drivers</li> <li>baseline creation</li> <li>how the model supports 3SF (Engagement\u2013Delivery\u2013Value)</li> </ul> <p>Purpose: Create a shared mental model across PM, EM, BA, PD, Architect, Dev, QA, and leadership.</p>"},{"location":"010-quick-start/#translation-map","title":"Translation Map","text":"<p>How all estimation methods connect to each other.</p> <p>This page explains how complexity &amp; uncertainty map to:</p> <ul> <li>RFP cost estimates</li> <li>story points &amp; slicing</li> <li>discovery &amp; design effort</li> <li>forecasting models</li> <li>value-based prioritization</li> <li>dependency &amp; NFR workloads</li> </ul> <p>Purpose: Give teams one \u201cRosetta Stone\u201d that aligns FTEs, SPs, velocity, forecasting, and value.</p>"},{"location":"010-quick-start/#scoring-sheet-framework","title":"Scoring Sheet Framework","text":"<p>How teams quantify complexity &amp; uncertainty in practice.</p> <p>This page defines:</p> <ul> <li>scoring mechanics</li> <li>baseline creation</li> <li>drift tracking</li> <li>uncertainty burn-down</li> <li>mapping complexity to story points</li> <li>structure for the new Excel tool</li> </ul> <p>Purpose: Provide a consistent operational method to measure and track scope across RFP \u2192 Discovery \u2192 Delivery.</p>"},{"location":"010-quick-start/#estimation-delivery-diagnostic","title":"Estimation &amp; Delivery Diagnostic","text":"<p>How to identify where the estimation system is breaking \u2014 and why.</p> <p>This page gives:</p> <ul> <li>a 4-domain diagnostic</li> <li>scoring model</li> <li>health index</li> <li>common failure patterns</li> <li>root causes</li> <li>recommendations linked to 3SF lines</li> </ul> <p>Purpose: Offer PMs, EMs, Directors, and Practice Leads a fast way to detect misalignment and intervene early.</p>"},{"location":"010-quick-start/#implementation-stages-plan","title":"Implementation Stages Plan","text":"<p>How to adopt this model across the organization.</p> <p>This page includes:</p> <ul> <li>rollout strategy</li> <li>pilot selection</li> <li>refinement loops</li> <li>communication plan</li> <li>RACI</li> <li>risk mitigation</li> <li>timeline</li> <li>institutionalization plan</li> </ul> <p>Purpose: Ensure sustainable, low-friction adoption across teams, portfolios, and leadership levels.</p>"},{"location":"010-quick-start/#who-should-use-this-hub","title":"Who Should Use This Hub","text":"<p>This hub is designed for:</p> <ul> <li>Engineering Directors</li> <li>Practice Leads (Engineering, PM, PD, BA, QA, Architecture)</li> <li>Project Leads</li> <li>Engineering Managers</li> <li>Product Designers</li> <li>Business Analysts</li> <li>Architects</li> <li>Senior Developers</li> <li>Anyone involved in estimation, scoping, planning, or forecasting</li> </ul>"},{"location":"010-quick-start/#how-to-navigate","title":"How to Navigate","text":"<p>Use the pages in the following sequence:</p> <ol> <li>Start with The Model \u2192 understand the model</li> <li>Move to The Map \u2192 understand the translation relationships</li> <li>Review The Scoring \u2192 understand scoring mechanics and baseline creation</li> <li>Use The Diagnostic \u2192 assess current project health</li> <li>Share The Plan \u2192 plan adoption with your leadership</li> <li>Read The FAQ \u2192 find answers from different perspectives</li> </ol> <p>This sequence mirrors the natural flow of learning and change.</p>"},{"location":"010-quick-start/#what-this-enables","title":"What This Enables","text":"<p>Once adopted, this model unlocks:</p> <ul> <li>predictable delivery</li> <li>coherent estimation culture</li> <li>consistent upstream and downstream alignment</li> <li>early detection of drift</li> <li>fewer escalations</li> <li>stronger client relationships</li> <li>better portfolio-level visibility</li> <li>alignment across the 3SF Engagement\u2013Delivery\u2013Value loop</li> </ul> <p>It becomes a long-term capability inside the organization \u2014 not a one-time process change.</p>"},{"location":"100-model/","title":"Complexity &amp; Uncertainty Model for Estimation Alignment","text":"<p>A shared model for improving predictability, reducing risk, and aligning estimation across all stages of delivery.</p>"},{"location":"100-model/#1-purpose-of-this-model","title":"1. Purpose of This Model","text":"<p>Software delivery uses multiple estimation methods \u2014 FTEs, story points, velocity, sprints, cost models, risk modifiers. Each method has a valid use case, but they rarely align. This creates:</p> <ul> <li>inconsistent expectations</li> <li>hidden complexity growth</li> <li>inaccurate timelines</li> <li>last-minute scope cuts</li> <li>budget overruns</li> <li>pressure on teams to \u201cincrease velocity\u201d instead of improving clarity</li> </ul> <p>This model introduces a shared foundation based on Complexity and Uncertainty, so every estimation method finally connects and supports predictable delivery.</p>"},{"location":"100-model/#core-message-to-communicate-across-roles","title":"Core Message (to communicate across roles)","text":"<p>We are not changing our estimation methods. We are aligning them through a shared complexity &amp; uncertainty model that improves predictability, reduces risk, and creates transparency across the lifecycle.</p> <p>This model fits the 3SF (3-in-3 SDLC Framework) by strengthening all three connection lines:</p> <ul> <li>Engagement \u2192 more accurate RFP estimation</li> <li>Delivery \u2192 transparent tracking of complexity drift</li> <li>Value \u2192 clearer trade-offs and prioritization</li> </ul>"},{"location":"100-model/#2-why-estimation-fails-today","title":"2. Why Estimation Fails Today","text":"<p>Across many projects, the same blind spots appear:</p>"},{"location":"100-model/#a-initial-estimations-focus-almost-entirely-on-development-effort","title":"A. Initial estimations focus almost entirely on development effort","text":"<p>Discovery, design, architecture, alignment work, and dependencies are often assumed as \u201cincluded\u201d but not quantified.</p>"},{"location":"100-model/#b-roles-that-carry-early-stage-complexity-are-not-estimating-it","title":"B. Roles that carry early-stage complexity are not estimating it","text":"<p>PMs, BAs, PDs, and Architects are rarely involved in defining early-phase complexity during RFPs.</p>"},{"location":"100-model/#c-delivery-uses-a-different-measurement-system","title":"C. Delivery uses a different measurement system","text":"<p>Initial estimate = FTE \u00d7 weeks Delivery = story points, velocity, backlog slicing These worlds do not talk to each other.</p>"},{"location":"100-model/#d-complexity-grows-silently-because-no-one-measures-it","title":"D. Complexity grows silently because no one measures it","text":"<p>Teams feel the drift but cannot quantify it. This leads to pressure on velocity instead of early re-alignment.</p>"},{"location":"100-model/#e-no-shared-baseline-exists-for-comparison","title":"E. No shared baseline exists for comparison","text":"<p>Without a complexity baseline, nobody can see when reality diverges from assumptions.</p> <p>This model directly addresses these gaps.</p>"},{"location":"100-model/#3-the-two-missing-dimensions","title":"3. The Two Missing Dimensions","text":""},{"location":"100-model/#1-complexity-the-amount-of-known-work","title":"1. Complexity = the amount of known work","text":"<p>This includes integration challenges, data structures, UI flows, dependencies, NFRs, domain logic, and more.</p>"},{"location":"100-model/#2-uncertainty-the-amount-of-unknown-work","title":"2. Uncertainty = the amount of unknown work","text":"<p>This includes unclear requirements, unvalidated assumptions, weak documentation, unclear ownership, and ambiguous dependencies.</p> <p>Both dimensions must be explicitly estimated at three points:</p> <ul> <li>During RFP (assumptions)</li> <li>During Discovery/Design (clarification)</li> <li>During Delivery (actual drift)</li> </ul>"},{"location":"100-model/#4-the-lifecycle-model-aligned-with-3sf","title":"4. The Lifecycle Model (Aligned With 3SF)","text":""},{"location":"100-model/#stage-1-engagement-rfp-pre-sales","title":"Stage 1. Engagement (RFP / Pre-sales)","text":"<p>Goal: Create a costable and defensible estimate. Tools: complexity scoring, uncertainty scoring, scenario ranges, risk multipliers.</p>"},{"location":"100-model/#stage-2-discovery-design","title":"Stage 2. Discovery &amp; Design","text":"<p>Goal: Turn ambiguity into clarity. Tools: complexity re-scoring, uncertainty burn-down, architecture/design ideation.</p>"},{"location":"100-model/#stage-3-delivery","title":"Stage 3. Delivery","text":"<p>Goal: Deliver predictably. Tools: story mapping, story point alignment, complexity-to-slice mapping, drift tracking.</p>"},{"location":"100-model/#stage-4-forecasting-trade-offs","title":"Stage 4. Forecasting &amp; Trade-offs","text":"<p>Goal: Make informed decisions when conditions change. Tools: updated complexity totals, Monte Carlo projections, scenario modeling.</p>"},{"location":"100-model/#stage-5-value-realization-next-phases","title":"Stage 5. Value Realization &amp; Next Phases","text":"<p>Goal: Maximize business value within constraints. Tools: value vs complexity matrix, de-scope guidance, opportunity sizing.</p>"},{"location":"100-model/#5-roles-responsibilities-across-stages","title":"5. Roles &amp; Responsibilities Across Stages","text":""},{"location":"100-model/#pre-sales-rfp","title":"Pre-sales / RFP","text":"<ul> <li>Engineering: estimate development complexity</li> <li>PM / BA: estimate discovery &amp; requirement complexity</li> <li>PD / UX: estimate design complexity</li> <li>Architect: estimate integration, data, and NFR complexity</li> <li>Practice Leads: validate feasibility &amp; team composition</li> </ul>"},{"location":"100-model/#discovery-design","title":"Discovery &amp; Design","text":"<ul> <li>All above roles refine complexity &amp; reduce uncertainty</li> <li>Delivery leads establish a complexity baseline for execution</li> </ul>"},{"location":"100-model/#delivery","title":"Delivery","text":"<ul> <li>Team decomposes work into slices/story points</li> <li>Track complexity drift every sprint</li> <li>PM/PL owns alignment with budget/timeline</li> <li>BA/PD/Architect maintain scope clarity</li> </ul>"},{"location":"100-model/#forecasting","title":"Forecasting","text":"<ul> <li>Leads review complexity changes</li> <li>Directors review trade-off options</li> </ul>"},{"location":"100-model/#6-the-complexity-driver-set-with-scoring-scale","title":"6. The Complexity Driver Set (with scoring scale)","text":"<p>All drivers use the same numeric scale:</p> <ul> <li>0 = Not applicable</li> <li>1 = Low impact</li> <li>2 = Medium impact</li> <li>3 = High impact</li> <li>5 = Extreme (transformational impact)</li> </ul>"},{"location":"100-model/#drivers-example-set","title":"Drivers (example set):","text":"<ol> <li>Integration complexity</li> <li>Data complexity</li> <li>UI/UX complexity</li> <li>Business logic complexity</li> <li>Security/compliance</li> <li>Non-functional requirements</li> <li>Team capability/fit</li> <li>External dependencies</li> <li>Environment/DevOps maturity</li> </ol> <p>The sum forms the Complexity Score.</p>"},{"location":"100-model/#7-the-uncertainty-driver-set","title":"7. The Uncertainty Driver Set","text":"<p>Same scale (0\u20135). Examples:</p> <ol> <li>Requirement clarity</li> <li>Stakeholder alignment</li> <li>Dependency ownership clarity</li> <li>Missing documentation</li> <li>Ambiguous domain logic</li> <li>Need for spikes/PoCs</li> <li>Historical unpredictability</li> </ol> <p>The sum forms the Uncertainty Score.</p>"},{"location":"100-model/#8-the-complexity-baseline","title":"8. The Complexity Baseline","text":"<p>The baseline is created during RFP by summing:</p> <ul> <li>Complexity Score (per feature/capability)</li> <li>Uncertainty Score</li> <li>Estimated effort per complexity segment</li> </ul> <p>This creates:</p> <ul> <li>a shared reference point</li> <li>a measurable \u201cbudget of complexity\u201d</li> <li>a way to detect drift early</li> </ul> <p>This baseline must be scored again during Discovery and Delivery.</p>"},{"location":"100-model/#9-complexity-drift-the-missing-metric","title":"9. Complexity Drift (the missing metric)","text":""},{"location":"100-model/#definition","title":"Definition:","text":"<p>The difference between original complexity assumptions and current complexity scores during delivery.</p> <p>This enables the team to say:</p> <ul> <li>\u201cWe discovered 30% more complexity in integrations.\u201d</li> <li>\u201cUncertainty burn-down was incomplete.\u201d</li> <li>\u201cScope change occurred earlier than expected.\u201d</li> <li>\u201cVelocity is not the issue \u2014 complexity expanded.\u201d</li> </ul> <p>This is where estimation finally becomes predictable.</p>"},{"location":"100-model/#10-how-this-model-aligns-all-estimation-methods","title":"10. How This Model Aligns All Estimation Methods","text":""},{"location":"100-model/#rfp-costing","title":"RFP \u2192 Costing","text":"<p>Complexity \u00d7 FTE multipliers Uncertainty \u00d7 risk multipliers PERT ranges (best/likely/worst)</p>"},{"location":"100-model/#discovery-breakdown","title":"Discovery \u2192 Breakdown","text":"<p>Complexity \u2192 design &amp; discovery workload Uncertainty \u2192 spikes &amp; clarifications</p>"},{"location":"100-model/#delivery-task-level-work","title":"Delivery \u2192 Task-level work","text":"<p>Complexity \u2192 story point ranges Uncertainty \u2192 backlog readiness Baseline \u2192 sprint scope health</p>"},{"location":"100-model/#forecasting-predictability","title":"Forecasting \u2192 Predictability","text":"<p>Complexity \u2191 \u2192 batch size \u2191 \u2192 cycle time \u2191 Uncertainty \u2191 \u2192 variance \u2191 \u2192 broader forecasting ranges</p>"},{"location":"100-model/#trade-offs-value-discussions","title":"Trade-offs \u2192 Value discussions","text":"<p>High-value, low-complexity \u2192 priority Low-value, high-complexity \u2192 cut or delay</p> <p>No more \u201cwhy are we behind?\u201d Now: \u201cwhere did complexity diverge from assumption?\u201d</p>"},{"location":"100-model/#11-questions-people-will-ask-and-the-real-answers","title":"11. Questions People Will Ask (and the Real Answers)","text":""},{"location":"100-model/#q1-what-estimation-method-should-we-use","title":"Q1: What estimation method should we use?","text":"<p>A: All methods remain. This model ensures they align.</p>"},{"location":"100-model/#q2-will-this-slow-us-down","title":"Q2: Will this slow us down?","text":"<p>A: No \u2014 it prevents months of drift and rework.</p>"},{"location":"100-model/#q3-doesnt-this-create-extra-work","title":"Q3: Doesn\u2019t this create extra work?","text":"<p>A: Only upfront. It eliminates late-stage chaos and escalations.</p>"},{"location":"100-model/#q4-does-this-replace-story-points","title":"Q4: Does this replace story points?","text":"<p>A: No. Story points stay \u2014 they now map to a complexity baseline.</p>"},{"location":"100-model/#q5-who-maintains-complexity-scoring","title":"Q5: Who maintains complexity scoring?","text":"<p>A: PM/BA/PD/Architect + Team Leads, not just engineers.</p>"},{"location":"100-model/#q6-why-do-we-need-both-complexity-and-uncertainty","title":"Q6: Why do we need both complexity and uncertainty?","text":"<p>A:</p> <ul> <li>Complexity = known scope</li> <li>Uncertainty = unknown scope   Mixing them is the root cause of underestimation.</li> </ul>"},{"location":"100-model/#q7-is-this-aligned-with-3sf","title":"Q7: Is this aligned with 3SF?","text":"<p>A:</p> <ul> <li>Engagement becomes more accurate</li> <li>Delivery becomes predictable</li> <li>Value conversations become factual</li> </ul>"},{"location":"110-map/","title":"Translation Map: Connecting Estimation Methods Across the Delivery Lifecycle","text":"<p>A unified framework that aligns estimation for RFP, discovery, delivery, forecasting, and value.</p>"},{"location":"110-map/#1-purpose-of-this-translation-map","title":"1. Purpose of This Translation Map","text":"<p>Every stage of the lifecycle uses a different estimation method:</p> <ul> <li>Pre-sales \u2192 FTE &amp; cost</li> <li>Delivery \u2192 story points &amp; velocity</li> <li>Forecasting \u2192 flow metrics</li> <li>Trade-offs \u2192 value vs. effort</li> <li>Design/Architecture \u2192 discovery workload</li> </ul> <p>Historically, these methods do not align, creating:</p> <ul> <li>mismatched expectations</li> <li>drift between RFP and delivery</li> <li>inflated velocity</li> <li>late scope cuts</li> <li>unpredictable timelines</li> </ul> <p>This map creates a single coherent translation layer grounded in Complexity and Uncertainty, so each method reinforces the others.</p>"},{"location":"110-map/#2-the-translation-challenge-why-we-need-this-map","title":"2. The Translation Challenge (Why We Need This Map)","text":"<p>Common questions teams face:</p> <ul> <li>\u201cHow does the cost estimate connect to story points?\u201d</li> <li>\u201cWhy does velocity go up but timeline still slips?\u201d</li> <li>\u201cWhy does RFP complexity disappear during delivery?\u201d</li> <li>\u201cWhy is design always under-estimated?\u201d</li> <li>\u201cHow do we detect when scope has actually changed?\u201d</li> </ul> <p>This map answers them.</p>"},{"location":"110-map/#3-the-core-model-two-dimensions-three-outputs","title":"3. The Core Model: Two Dimensions, Three Outputs","text":""},{"location":"110-map/#input-dimensions","title":"Input Dimensions","text":"<ol> <li>Complexity (known work)</li> <li>Uncertainty (unknown work)</li> </ol>"},{"location":"110-map/#three-outputs","title":"Three Outputs","text":"<ol> <li>Initial effort (RFP)</li> <li>Delivery effort (story points &amp; flow)</li> <li>Forecasting (probabilistic timelines)</li> </ol> <p>These outputs must be derived from the same backbone.</p>"},{"location":"110-map/#4-mapping-complexity-to-different-estimation-methods","title":"4. Mapping Complexity to Different Estimation Methods","text":"<p>Below are the most common methods in your organization and how complexity connects them.</p>"},{"location":"110-map/#41-complexity-fte-estimates-rfp-stage","title":"4.1. Complexity \u2192 FTE Estimates (RFP stage)","text":"<p>At RFP, we estimate cost in FTE-weeks/months.</p>"},{"location":"110-map/#translation","title":"Translation:","text":"<p>Complexity Score determines the multiplier applied to base FTE effort.</p> <p>Example multiplier table:</p> Complexity Sum Multiplier 0\u20133 \u00d71.0 4\u20137 \u00d71.2 8\u201312 \u00d71.5 13+ \u00d72.0"},{"location":"110-map/#example","title":"Example:","text":"<ul> <li>Base effort = 2 dev-weeks</li> <li>Complexity = 10 \u2192 multiplier \u00d71.5</li> <li>Total = 3 dev-weeks (before uncertainty)</li> </ul> <p>This is clean, defensible, and scalable.</p>"},{"location":"110-map/#42-complexity-story-points-delivery-stage","title":"4.2. Complexity \u2192 Story Points (Delivery stage)","text":"<p>Story points don\u2019t disappear. They become a refinement of the complexity baseline.</p>"},{"location":"110-map/#translation_1","title":"Translation:","text":"<p>Complexity levels anchor SP ranges.</p> Complexity Level SP Range Low (1\u20133) 1\u20133 Medium (4\u20137) 5\u20138 High (8\u201312) 13\u201321 Extreme (13+) &gt;21 or split <p>This prevents silent scope inflation hidden inside SP re-estimation.</p>"},{"location":"110-map/#43-complexity-team-composition-skills","title":"4.3. Complexity \u2192 Team Composition &amp; Skills","text":"<p>A high complexity driver such as:</p> <ul> <li>integration</li> <li>domain logic</li> <li>NFR</li> <li>compliance</li> <li>unclear ownership</li> </ul> <p>\u2026means we need correct staffing early.</p>"},{"location":"110-map/#translation-examples","title":"Translation examples:","text":"<ul> <li>High integration complexity \u2192 Architect involvement full cycle</li> <li>High UX complexity \u2192 PD workload increases</li> <li>High domain logic complexity \u2192 BA workload increases</li> </ul> <p>This corrects the blind spot where PM/PD/BA effort is assumed rather than estimated.</p>"},{"location":"110-map/#44-complexity-design-discovery-effort","title":"4.4. Complexity \u2192 Design &amp; Discovery Effort","text":"<p>Discovery is not \u201coverhead.\u201d It is complexity translation.</p>"},{"location":"110-map/#translation_2","title":"Translation:","text":"<ul> <li>High domain complexity \u2192 longer BA clarification</li> <li>High UX complexity \u2192 more PD design cycles</li> <li>High integration complexity \u2192 architecture mapping</li> <li>High uncertainty \u2192 dependency mapping, spikes</li> </ul> <p>This links discovery/design to reality instead of guessing.</p>"},{"location":"110-map/#5-mapping-uncertainty-across-methods","title":"5. Mapping Uncertainty Across Methods","text":"<p>Uncertainty is a distinct dimension. It affects:</p> <ul> <li>risk</li> <li>confidence</li> <li>cost buffers</li> <li>timeline ranges</li> <li>probability of scope drift</li> </ul>"},{"location":"110-map/#51-uncertainty-risk-modifiers-rfp","title":"5.1. Uncertainty \u2192 Risk Modifiers (RFP)","text":"<p>Example multipliers:</p> Uncertainty Score Multiplier 0\u20131 \u00d71.0 2 \u00d71.3 3 \u00d71.6 5 \u00d72.0"},{"location":"110-map/#example_1","title":"Example:","text":"<p>Effort after complexity = 3 dev-weeks Uncertainty multiplier = \u00d71.6 Final = 4.8 \u2192 round to 5 dev-weeks</p>"},{"location":"110-map/#52-uncertainty-pert-ranges-best-likely-worst","title":"5.2. Uncertainty \u2192 PERT Ranges (best / likely / worst)","text":"<p>Uncertainty maps directly to variance.</p> <ul> <li>Low uncertainty \u2192 tight range</li> <li>High uncertainty \u2192 wide range</li> </ul> <p>Example:</p> <ul> <li>Likely: 5 weeks</li> <li>Low U \u2192 4\u20136</li> <li>Medium U \u2192 3\u20138</li> <li>High U \u2192 2\u201312</li> </ul> <p>This avoids the lie of a single-scenario estimate.</p>"},{"location":"110-map/#53-uncertainty-discovery-workload-delivery-readiness","title":"5.3. Uncertainty \u2192 Discovery Workload (delivery readiness)","text":"<p>High uncertainty automatically means:</p> <ul> <li>spikes</li> <li>PoCs</li> <li>stakeholder sessions</li> <li>alignment workshops</li> <li>design explorations</li> </ul> <p>Teams can no longer skip clarification work.</p>"},{"location":"110-map/#54-uncertainty-probability-of-drift","title":"5.4. Uncertainty \u2192 Probability of Drift","text":"<p>This becomes the early-warning indicator.</p> <p>If uncertainty stays high during delivery \u2192 timeline will slip.</p>"},{"location":"110-map/#6-combining-complexity-uncertainty-across-methods","title":"6. Combining Complexity + Uncertainty Across Methods","text":""},{"location":"110-map/#combined-matrix","title":"Combined matrix:","text":"Complexity \u2192 Low (1\u20133) Medium (4\u20137) High (8\u201312) Extreme (13+) Low U predictable predictable slower but stable stable if well-staffed Medium U mild drift moderate drift large drift almost guaranteed change High U severe drift severe drift major re-estimation not feasible <p>This table explains \u201cwhy projects slip\u201d without blaming velocity.</p>"},{"location":"110-map/#7-mapping-to-forecasting","title":"7. Mapping to Forecasting","text":""},{"location":"110-map/#complexity-batch-size","title":"Complexity \u2192 batch size","text":"<p>Higher complexity = larger stories = larger cycle time</p>"},{"location":"110-map/#uncertainty-variance","title":"Uncertainty \u2192 variance","text":"<p>Higher uncertainty = wider spread in Monte Carlo simulation</p>"},{"location":"110-map/#complexity-baseline-throughput","title":"Complexity baseline \u2192 throughput","text":"<p>Provides a clear comparison of planned vs. actual</p>"},{"location":"110-map/#complexity-drift-early-warning-signals","title":"Complexity drift \u2192 early-warning signals","text":"<p>If drift grows faster than throughput, the project is behind.</p>"},{"location":"110-map/#8-mapping-to-value-trade-offs","title":"8. Mapping to Value &amp; Trade-Offs","text":"<p>This model makes prioritization empirical:</p>"},{"location":"110-map/#high-value-low-complexity-do-immediately","title":"High-value, low-complexity \u2192 do immediately","text":""},{"location":"110-map/#high-value-high-complexity-invest-carefully-scope-slices","title":"High-value, high-complexity \u2192 invest carefully / scope slices","text":""},{"location":"110-map/#low-value-high-complexity-cut-early","title":"Low-value, high-complexity \u2192 cut early","text":""},{"location":"110-map/#low-value-low-complexity-opportunistic","title":"Low-value, low-complexity \u2192 opportunistic","text":"<p>This matches the 3SF value cycle (product \u2194 client).</p>"},{"location":"110-map/#9-mapping-to-3sf-3-in-3-sdlc-framework","title":"9. Mapping to 3SF (3-in-3 SDLC Framework)","text":""},{"location":"110-map/#engagement-line-client-vendor","title":"Engagement Line (client \u2194 vendor)","text":"<ul> <li>Complexity &amp; Uncertainty create transparent RFP conversations</li> <li>Fewer surprises</li> <li>More realistic proposals</li> <li>Trust increases</li> </ul>"},{"location":"110-map/#delivery-line-vendor-product","title":"Delivery Line (vendor \u2194 product)","text":"<ul> <li>Story points become meaningful</li> <li>Velocity pressure decreases</li> <li>Complexity drift becomes visible</li> <li>Predictability increases</li> </ul>"},{"location":"110-map/#value-line-product-client","title":"Value Line (product \u2194 client)","text":"<ul> <li>Tradeoffs become transparent</li> <li>Prioritization becomes data-driven</li> <li>Scope decisions align to business outcomes</li> <li>Value delivery becomes measurable</li> </ul> <p>This model strengthens all 3 connections in the 3SF triangle.</p>"},{"location":"110-map/#10-faq-what-this-page-answers","title":"10. FAQ: What This Page Answers","text":""},{"location":"110-map/#q-how-does-the-rfp-estimate-connect-to-story-points","title":"Q: How does the RFP estimate connect to story points?","text":"<p>A: Through complexity mapping. They share the same backbone.</p>"},{"location":"110-map/#q-why-does-velocity-grow-but-timeline-still-slip","title":"Q: Why does velocity grow but timeline still slip?","text":"<p>A: Because complexity drift remained invisible. Velocity cannot compensate for scope inflation.</p>"},{"location":"110-map/#q-how-do-we-detect-scope-change-early","title":"Q: How do we detect scope change early?","text":"<p>A: Re-score complexity each sprint. Compare to the baseline.</p>"},{"location":"110-map/#q-how-do-we-prioritize-effectively","title":"Q: How do we prioritize effectively?","text":"<p>A: Use the value \u00d7 complexity matrix.</p>"},{"location":"110-map/#q-does-this-replace-existing-tools","title":"Q: Does this replace existing tools?","text":"<p>A: No. It aligns them.</p>"},{"location":"110-map/#11-next-page","title":"11. Next Page","text":"<p>Next we create:</p>"},{"location":"110-map/#page-3-scoring-sheet-model-tool","title":"Page 3: Scoring Sheet (Model + Tool)","text":"<p>It will include:</p> <ul> <li>complexity scoring</li> <li>uncertainty scoring</li> <li>complexity baseline creation</li> <li>drift tracking</li> <li>refinements during delivery</li> <li>Excel/Confluence versions</li> <li>integration with project dashboards</li> </ul>"},{"location":"120-scoring/","title":"Complexity &amp; Uncertainty Scoring Framework","text":"<p>A practical method to quantify scope, detect drift early, and link RFP \u2192 Discovery \u2192 Delivery.</p>"},{"location":"120-scoring/#1-purpose-of-the-scoring-sheet","title":"1. Purpose of the Scoring Sheet","text":"<p>The scoring sheet is the operational core of the estimation alignment framework. It brings consistency and transparency across stages by enabling teams to:</p> <ul> <li>quantify complexity</li> <li>quantify uncertainty</li> <li>create an initial estimation baseline</li> <li>track complexity &amp; uncertainty drift during delivery</li> <li>understand when and why assumptions change</li> <li>provide early signals for timeline/budget adjustments</li> <li>enable informed trade-off decisions</li> </ul> <p>This sheet becomes the single source of truth connecting RFP, discovery, sprint planning, and forecasting.</p>"},{"location":"120-scoring/#2-when-the-scoring-sheet-is-used-lifecycle","title":"2. When the Scoring Sheet Is Used (Lifecycle)","text":""},{"location":"120-scoring/#stage-1-pre-sales-rfp","title":"Stage 1 \u2014 Pre-sales / RFP","text":"<p>Used to:</p> <ul> <li>score high-level features/capabilities</li> <li>estimate initial complexity</li> <li>estimate uncertainty that impacts costing</li> <li>generate PERT ranges</li> <li>apply risk multipliers</li> <li>create the Complexity Baseline v1</li> </ul>"},{"location":"120-scoring/#stage-2-discovery-design","title":"Stage 2 \u2014 Discovery &amp; Design","text":"<p>Used to:</p> <ul> <li>re-score complexity with clearer information</li> <li>measure uncertainty burn-down</li> <li>surface discovery/design workload</li> <li>adjust the baseline to v2</li> </ul>"},{"location":"120-scoring/#stage-3-delivery","title":"Stage 3 \u2014 Delivery","text":"<p>Used to:</p> <ul> <li>map complexity to story points</li> <li>detect complexity drift</li> <li>track uncertainty changes</li> <li>adjust forecasts</li> <li>give early signals to PM/PL</li> <li>maintain Baseline v3+ as needed</li> </ul>"},{"location":"120-scoring/#stage-4-forecasting-trade-offs","title":"Stage 4 \u2014 Forecasting &amp; Trade-Offs","text":"<p>Used for:</p> <ul> <li>Monte Carlo inputs</li> <li>new scope items</li> <li>scope reduction analysis</li> <li>value/complexity prioritization</li> <li>stakeholder communication</li> </ul>"},{"location":"120-scoring/#3-what-gets-scored-granularity-levels","title":"3. What Gets Scored (Granularity Levels)","text":"<p>Depending on the stage, scoring can occur at:</p>"},{"location":"120-scoring/#rfp","title":"RFP:","text":"<ul> <li>EPICs</li> <li>high-level features</li> <li>integration blocks</li> <li>capabilities</li> <li>non-functional requirement clusters</li> </ul>"},{"location":"120-scoring/#discovery","title":"Discovery:","text":"<ul> <li>decomposed features</li> <li>UX flows / screens</li> <li>data flows</li> <li>API surfaces</li> <li>domain logic segments</li> </ul>"},{"location":"120-scoring/#delivery","title":"Delivery:","text":"<ul> <li>stories (when needed)</li> <li>slices / increments</li> <li>integration tasks</li> <li>cross-team dependencies</li> <li>scope changes</li> </ul> <p>Important: teams should not score every story. They score units of meaning (EPICs/features/slices) \u2014 not tasks.</p>"},{"location":"120-scoring/#4-how-to-score-complexity-drivers-scale","title":"4. How to Score Complexity (Drivers + Scale)","text":"<p>All drivers use the same scale:</p> Score Meaning 0 Not applicable 1 Low complexity 2 Medium complexity 3 High complexity 5 Extreme complexity (red flag) <p>The nine recommended drivers:</p> <ol> <li>Integration complexity</li> <li>Data complexity</li> <li>UI/UX complexity</li> <li>Business logic complexity</li> <li>Security/compliance requirements</li> <li>Non-functional requirements</li> <li>Team capability/fit</li> <li>External dependencies</li> <li>Environment/DevOps maturity</li> </ol>"},{"location":"120-scoring/#complexity-score-sum-of-relevant-driver-scores","title":"Complexity Score = sum of relevant driver scores","text":""},{"location":"120-scoring/#5-how-to-score-uncertainty","title":"5. How to Score Uncertainty","text":"<p>Same scale (0\u20135). Common drivers:</p> <ol> <li>Requirements clarity</li> <li>Stakeholder alignment</li> <li>Domain ambiguity</li> <li>Dependency ownership</li> <li>Availability of documentation</li> <li>Need for spikes/PoCs</li> <li>Unpredictable history</li> <li>Cross-team coordination unknowns</li> <li>Client-side processes unknowns</li> </ol>"},{"location":"120-scoring/#uncertainty-score-sum-of-uncertainty-driver-scores","title":"Uncertainty Score = sum of uncertainty driver scores","text":""},{"location":"120-scoring/#6-creating-the-complexity-baseline-rfp-discovery","title":"6. Creating the Complexity Baseline (RFP \u2192 Discovery)","text":"<p>The baseline is a table of:</p> <p>| Capability/Feature | Complexity Score | Uncertainty Score | Combined Initial Score | Expected Effort (weeks/FTEs) |</p> <p>The \u201cexpected effort\u201d is generated using:</p> <ul> <li>complexity multipliers</li> <li>uncertainty multipliers</li> <li>PERT ranges</li> </ul> <p>This baseline becomes the agreed assumption set for RFP.</p>"},{"location":"120-scoring/#baseline-v1-rfp","title":"Baseline v1 (RFP):","text":"<ul> <li>Highest uncertainty</li> <li>Coarse-grained features</li> <li>Good for costing</li> <li>Not accurate enough for planning</li> </ul>"},{"location":"120-scoring/#baseline-v2-post-discovery","title":"Baseline v2 (Post-Discovery):","text":"<ul> <li>Lower uncertainty</li> <li>Refined features</li> <li>Better complexity accuracy</li> <li>Used for delivery planning</li> </ul>"},{"location":"120-scoring/#baseline-v3-delivery-optional","title":"Baseline v3 (Delivery, optional):","text":"<ul> <li>Only updated if major complexity drift occurs</li> <li>Controls escalation and expectations</li> </ul>"},{"location":"120-scoring/#7-mapping-scores-to-delivery-story-points-slices","title":"7. Mapping Scores to Delivery (Story Points &amp; Slices)","text":"<p>The sheet provides a mapping:</p> Complexity Score Interpretation Suggested SP Range 1\u20133 (Low) Simple 1\u20133 points 4\u20137 (Medium) Straightforward but multi-step 5\u20138 points 8\u201312 (High) Difficult or multi-domain 13\u201321 points 13+ (Extreme) Break into smaller slices &gt;21 or split <p>This prevents hidden re-estimation and velocity manipulation.</p>"},{"location":"120-scoring/#8-tracking-complexity-drift-delivery-stage","title":"8. Tracking Complexity Drift (Delivery Stage)","text":"<p>The scoring sheet includes:</p> <ul> <li>Current Complexity Score</li> <li>Difference vs Baseline</li> <li>Percentage drift (+/-)</li> <li>Drift categories (integration, UX, dependencies, NFRs\u2026)</li> <li>Impact indicators (timeline, capacity, budget)</li> </ul>"},{"location":"120-scoring/#example-drift-classification","title":"Example Drift Classification","text":"Drift Meaning Action 0\u201310% Normal variation No action needed 10\u201325% Manageable drift Re-plan slices, inform stakeholders 25\u201340% Significant drift Align scope, schedule trade-offs 40%+ Critical drift Executive attention, contract adjustment <p>This replaces emotional escalations with clear data.</p>"},{"location":"120-scoring/#9-tracking-uncertainty-burn-down","title":"9. Tracking Uncertainty Burn-Down","text":"<p>Uncertainty should decrease as the project progresses.</p> <p>The sheet highlights whether it actually does.</p>"},{"location":"120-scoring/#if-uncertainty-stays-flat","title":"If uncertainty stays flat:","text":"<p>Discovery is incomplete \u2192 expect rework.</p>"},{"location":"120-scoring/#if-uncertainty-increases","title":"If uncertainty increases:","text":"<p>Hidden risks have surfaced \u2192 intervene early.</p>"},{"location":"120-scoring/#10-sheet-sections-structure-for-final-tool","title":"10. Sheet Sections (Structure for Final Tool)","text":""},{"location":"120-scoring/#section-a-featureepic-list","title":"Section A \u2014 Feature/EPIC List","text":"<ul> <li>list all items to be estimated</li> <li>define granularity level</li> </ul>"},{"location":"120-scoring/#section-b-complexity-driver-scoring","title":"Section B \u2014 Complexity Driver Scoring","text":"<ul> <li>matrix for scoring 1\u20139 drivers per feature</li> </ul>"},{"location":"120-scoring/#section-c-uncertainty-driver-scoring","title":"Section C \u2014 Uncertainty Driver Scoring","text":"<ul> <li>matrix for 1\u20139 uncertainty factors</li> </ul>"},{"location":"120-scoring/#section-d-combined-score","title":"Section D \u2014 Combined Score","text":"<ul> <li>weighted or additive depending on policy</li> </ul>"},{"location":"120-scoring/#section-e-rfp-effort-model","title":"Section E \u2014 RFP Effort Model","text":"<ul> <li>multipliers</li> <li>FTE + sprint translation</li> <li>PERT ranges</li> <li>preliminary cost</li> </ul>"},{"location":"120-scoring/#section-f-delivery-mapping","title":"Section F \u2014 Delivery Mapping","text":"<ul> <li>story point ranges</li> <li>batch-size implications</li> <li>cycle-time projections (optional)</li> </ul>"},{"location":"120-scoring/#section-g-baseline-comparison-drift-tracking","title":"Section G \u2014 Baseline Comparison &amp; Drift Tracking","text":"<ul> <li>baseline vs latest</li> <li>per-feature drift</li> <li>total drift</li> <li>drift by driver</li> <li>risk classification</li> </ul>"},{"location":"120-scoring/#section-h-trade-off-model","title":"Section H \u2014 Trade-Off Model","text":"<ul> <li>high-value/low-complexity items</li> <li>low-value/high-complexity candidates for removal</li> <li>fast-fix clusters</li> <li>long-tail clusters</li> </ul>"},{"location":"120-scoring/#11-practical-guidance-for-teams","title":"11. Practical Guidance for Teams","text":""},{"location":"120-scoring/#do-not-score-everything","title":"Do not \u201cscore everything.\u201d","text":"<p>Score meaningful units, not tasks.</p>"},{"location":"120-scoring/#do-not-treat-complexity-as-estimation","title":"Do not treat complexity as estimation.","text":"<p>It is a signal, not a precise timeline.</p>"},{"location":"120-scoring/#re-score-when-new-information-arrives","title":"Re-score when new information arrives.","text":"<p>Discovery &amp; dependencies change reality.</p>"},{"location":"120-scoring/#avoid-scoring-alone","title":"Avoid scoring alone.","text":"<p>Complexity is multi-role: BA + PD + Architect + EM + Lead.</p>"},{"location":"120-scoring/#dont-modify-story-points-to-hide-drift","title":"Don\u2019t modify story points to hide drift.","text":"<p>This sheet makes complexity visible \u2014 use it.</p>"},{"location":"120-scoring/#12-how-this-sheet-connects-to-the-overall-framework","title":"12. How This Sheet Connects to the Overall Framework","text":"<p>This page operationalizes:</p> <ul> <li>Page 1: The Model</li> <li>Page 2: The Translation Map</li> </ul> <p>The scoring sheet becomes the mechanical heart enabling:</p> <ul> <li>alignment</li> <li>traceability</li> <li>drift detection</li> <li>realistic forecasting</li> <li>transparent value decisions</li> <li>better client conversations</li> </ul> <p>It is the missing alignment tool between the three 3SF lines:</p> <ul> <li>Engagement \u2194 Delivery \u2194 Value</li> </ul>"},{"location":"130-diagnostic/","title":"Estimation &amp; Delivery Diagnostic","text":"<p>A structured way to identify where estimation alignment breaks, why drift happens, and how to intervene early.</p>"},{"location":"130-diagnostic/#1-purpose-of-the-diagnostic","title":"1. Purpose of the Diagnostic","text":"<p>This diagnostic exists to answer the question:</p> <p>\u201cWhere exactly is our estimation system breaking \u2014 and what can we do about it?\u201d</p> <p>Most projects feel symptoms like:</p> <ul> <li>slipping timelines</li> <li>increased velocity but decreasing predictability</li> <li>expanding scope</li> <li>chaotic refinement</li> <li>late discoveries</li> <li>unclear dependencies</li> <li>unexpected design/BA/architecture workload</li> <li>unclear trade-offs</li> </ul> <p>But without a shared model, teams cannot trace these symptoms back to causes.</p> <p>This diagnostic gives:</p> <ul> <li>clarity</li> <li>alignment</li> <li>a path to action</li> </ul> <p>It supports:</p> <ul> <li>PMs</li> <li>BAs</li> <li>PDs</li> <li>Architects</li> <li>Engineering Managers</li> <li>Leads</li> <li>Practice Leads</li> <li>Directors</li> </ul>"},{"location":"130-diagnostic/#2-how-to-use-this-diagnostic","title":"2. How to Use This Diagnostic","text":"<ul> <li>Run it at project start, post-discovery, and every 4\u20136 weeks.</li> <li>Each area is scored individually by key roles (PM, EM, PD, BA, Architect).</li> <li>Compare perceptions \u2014 misalignment between roles is itself a signal.</li> <li>Use results to guide interventions and stakeholder communication.</li> </ul>"},{"location":"130-diagnostic/#3-diagnostic-overview","title":"3. Diagnostic Overview","text":"<p>The diagnostic evaluates four domains:</p> <ol> <li>Engagement Alignment (RFP \u2192 Discovery)</li> <li>Delivery Preparedness (Discovery \u2192 Execution)</li> <li>Execution Health (Sprints \u2192 Outcomes)</li> <li>Value Realization (Delivery \u2192 Client Value)</li> </ol> <p>Each domain contains sub-metrics that map to the complexity/uncertainty model.</p>"},{"location":"130-diagnostic/#4-domain-1-engagement-alignment","title":"4. Domain 1 \u2014 Engagement Alignment","text":"<p>(Does our RFP assumption set reflect reality?)</p> <p>Score each item 0\u20133:</p> Score Meaning 0 Severe issue (red) 1 Weak alignment (yellow) 2 Mostly aligned (light green) 3 Fully aligned (green)"},{"location":"130-diagnostic/#metrics","title":"Metrics:","text":"<ol> <li> <p>RFP complexity scoring quality</p> </li> <li> <p>Was complexity scored?</p> </li> <li> <p>Was it done multi-role?</p> </li> <li> <p>Uncertainty scoring quality</p> </li> <li> <p>Were unknowns quantified?</p> </li> <li> <p>Early role involvement</p> </li> <li> <p>Were PM/BA/PD/Architect part of estimation, not just dev leads?</p> </li> <li> <p>Assumption clarity &amp; recording</p> </li> <li> <p>Are assumptions documented and visible?</p> </li> <li> <p>Feasibility of the proposed team composition</p> </li> <li> <p>Do assigned roles match complexity?</p> </li> <li> <p>Client-side dependencies identified?</p> </li> <li> <p>Or were they ignored?</p> </li> </ol>"},{"location":"130-diagnostic/#interpretation","title":"Interpretation:","text":"<ul> <li>0\u20138 points: High risk \u2014 RFP and reality misaligned</li> <li>9\u201314 points: Medium risk \u2014 recalibration needed</li> <li>15\u201318 points: Low risk \u2014 good alignment</li> </ul>"},{"location":"130-diagnostic/#5-domain-2-delivery-preparedness","title":"5. Domain 2 \u2014 Delivery Preparedness","text":"<p>(Is discovery turning complexity into clarity?)</p> <p>Score each item 0\u20133:</p> <ol> <li> <p>Discovery completeness</p> </li> <li> <p>Are requirements clarified before development?</p> </li> <li> <p>Is there a DoR-upstream model?</p> </li> <li> <p>Uncertainty burn-down</p> </li> <li> <p>Are unknowns decreasing over time?</p> </li> <li> <p>Are spikes/PoCs used effectively?</p> </li> <li> <p>Design readiness</p> </li> <li> <p>Are UX flows validated before implementation begins?</p> </li> <li> <p>Architecture clarity</p> </li> <li> <p>Are integration assumptions validated?</p> </li> <li> <p>APIs mapped? Ownership clear?</p> </li> <li> <p>BA requirement maturity</p> </li> <li> <p>Are acceptance criteria complete and stable?</p> </li> <li> <p>Cross-team dependency clarity</p> </li> <li> <p>Are SLAs known?</p> </li> <li> <p>Do we have predictable channels?</p> </li> <li> <p>Complexity baseline updated post-discovery</p> </li> <li> <p>Does Baseline v2 exist?</p> </li> <li>Does it reflect discovery findings?</li> </ol>"},{"location":"130-diagnostic/#interpretation_1","title":"Interpretation:","text":"<ul> <li>0\u201310 points: Insufficient discovery \u2192 expect significant drift</li> <li>11\u201316 points: Partial readiness \u2192 watch dependencies</li> <li>17\u201321 points: Good readiness</li> <li>22+ points: Strong, stable foundation</li> </ul>"},{"location":"130-diagnostic/#6-domain-3-execution-health","title":"6. Domain 3 \u2014 Execution Health","text":"<p>(Are we delivering predictably and transparently?)</p> <p>Score each item 0\u20133:</p> <ol> <li> <p>Complexity drift tracking</p> </li> <li> <p>Do we re-score complexity during delivery?</p> </li> <li> <p>Is drift visible?</p> </li> <li> <p>Story point consistency</p> </li> <li> <p>Are SP ranges anchored to complexity?</p> </li> <li> <p>No inflation? No gaming?</p> </li> <li> <p>Backlog health</p> </li> <li> <p>Does the team have 2\u20133 sprints ready?</p> </li> <li> <p>Is refinement consistent?</p> </li> <li> <p>Velocity stability</p> </li> <li> <p>Is velocity used properly (capacity signal, not performance)?</p> </li> <li> <p>Dependency management</p> </li> <li> <p>Are we blocked often?</p> </li> <li> <p>Are external dependencies predictable?</p> </li> <li> <p>Flow efficiency</p> </li> <li> <p>Are cycle times consistent with complexity?</p> </li> <li> <p>Scope control</p> </li> <li> <p>Are changes captured, scored, and visible to stakeholders?</p> </li> </ol>"},{"location":"130-diagnostic/#interpretation_2","title":"Interpretation:","text":"<ul> <li>0\u201310 points: Delivery in reactive mode \u2014 unstable</li> <li>11\u201316 points: Mixed signals \u2014 intervene early</li> <li>17\u201321 points: Predictable execution</li> <li>22+ points: Excellent health</li> </ul>"},{"location":"130-diagnostic/#7-domain-4-value-realization","title":"7. Domain 4 \u2014 Value Realization","text":"<p>(Are we delivering the right things in the right order?)</p> <p>Score each item 0\u20133:</p> <ol> <li> <p>Value vs. complexity prioritization</p> </li> <li> <p>Are we sequencing based on impact \u00d7 cost?</p> </li> <li> <p>Stakeholder engagement quality</p> </li> <li> <p>Are decisions transparent and predictable?</p> </li> <li> <p>Scope trade-offs clarity</p> </li> <li> <p>Does leadership understand complexity drift?</p> </li> <li> <p>Outcome alignment</p> </li> <li> <p>Are delivered increments tied to measurable value?</p> </li> <li> <p>Predictability of releases</p> </li> <li> <p>Are release plans stable and credible?</p> </li> </ol>"},{"location":"130-diagnostic/#interpretation_3","title":"Interpretation:","text":"<ul> <li>0\u20136 points: Low perceived value \u2014 major misalignment</li> <li>7\u201310 points: Partial value realization</li> <li>11\u201313 points: Strong alignment</li> <li>14\u201315 points: Very strong alignment</li> </ul>"},{"location":"130-diagnostic/#8-combined-diagnostic-score","title":"8. Combined Diagnostic Score","text":"Total Score Meaning 0\u201340 System in conflict mode (HCS + 3SF red zone) 41\u201360 System in reactive mode \u2014 needs structural fixes 61\u201375 System in alignment but with weak spots 76\u201390 Predictable, value-aligned delivery system <p>This becomes your organization\u2019s Estimation/Delivery Health Index.</p>"},{"location":"130-diagnostic/#9-diagnostic-patterns-what-the-scores-reveal","title":"9. Diagnostic Patterns (What the Scores Reveal)","text":""},{"location":"130-diagnostic/#pattern-a-high-execution-low-engagement","title":"Pattern A: High Execution, Low Engagement","text":"<p>Symptoms:</p> <ul> <li>Great delivery team</li> <li>Bad estimation foundation</li> <li>Sprints are healthy, project still slips</li> </ul> <p>Cause: RFP mismatch + hidden complexity</p>"},{"location":"130-diagnostic/#pattern-b-high-engagement-low-delivery","title":"Pattern B: High Engagement, Low Delivery","text":"<p>Symptoms:</p> <ul> <li>RFP clear</li> <li>Discovery good</li> <li>Execution chaotic</li> </ul> <p>Cause: No drift tracking + poor dependency management</p>"},{"location":"130-diagnostic/#pattern-c-high-discovery-low-value","title":"Pattern C: High Discovery, Low Value","text":"<p>Symptoms:</p> <ul> <li>Lots of clarity</li> <li>Lots of alignment</li> <li>But wrong things prioritized</li> </ul> <p>Cause: No value \u00d7 complexity prioritization</p>"},{"location":"130-diagnostic/#pattern-d-low-everything","title":"Pattern D: Low Everything","text":"<p>Symptoms:</p> <ul> <li>Estimation, discovery, delivery, value all unstable</li> </ul> <p>Cause: System in conflict mode (per HCS) Needs immediate intervention.</p>"},{"location":"130-diagnostic/#10-recommended-interventions-mapped-to-3sf","title":"10. Recommended Interventions (Mapped to 3SF)","text":"<p>Based on weaknesses:</p>"},{"location":"130-diagnostic/#engagement-line-client-vendor","title":"Engagement Line (client \u2194 vendor):","text":"<ul> <li>Re-score complexity at feature level</li> <li>Apply uncertainty scoring</li> <li>Add PM/BA/PD/Architect to RFP</li> <li>Capture assumptions in baseline</li> </ul>"},{"location":"130-diagnostic/#delivery-line-vendor-product","title":"Delivery Line (vendor \u2194 product):","text":"<ul> <li>Establish baseline v2</li> <li>Start drift tracking</li> <li>Anchor story points to complexity</li> <li>Improve backlog health</li> <li>Fix dependencies</li> </ul>"},{"location":"130-diagnostic/#value-line-product-client","title":"Value Line (product \u2194 client):","text":"<ul> <li>Use value \u00d7 complexity matrix</li> <li>Clarify sequencing intent</li> <li>Present trade-offs using drift numbers</li> <li>Align expectations on uncertainty</li> </ul>"},{"location":"130-diagnostic/#11-how-this-diagnostic-supports-your-rollout-strategy","title":"11. How This Diagnostic Supports Your Rollout Strategy","text":"<p>This diagnostic will be used:</p>"},{"location":"130-diagnostic/#by-practice-leads","title":"By Practice Leads","text":"<ul> <li>to align on estimation culture and expectations</li> </ul>"},{"location":"130-diagnostic/#by-engineering-directors","title":"By Engineering Directors","text":"<ul> <li>to assess team health</li> <li>to identify systemic issues across portfolios</li> </ul>"},{"location":"130-diagnostic/#by-project-leads-ems-pms","title":"By Project Leads / EMs / PMs","text":"<ul> <li>during escalations</li> <li>during planning resets</li> <li>during project reviews</li> <li>during quarterly portfolio assessments</li> </ul>"},{"location":"130-diagnostic/#by-teams","title":"By Teams","text":"<ul> <li>during retros</li> <li>during sprint boundaries</li> <li>during decomposition</li> <li>when feeling \u201clost\u201d</li> <li>when feeling pressure</li> </ul> <p>It becomes a shared language for everyone.</p>"},{"location":"140-plan/","title":"Implementation Stages Plan","text":"<p>A structured rollout of the Complexity &amp; Uncertainty Estimation Model across the organization.</p>"},{"location":"140-plan/#1-purpose-of-the-implementation-plan","title":"1. Purpose of the Implementation Plan","text":"<p>To successfully adopt the new estimation alignment model, the organization must intentionally introduce, socialize, pilot, refine, and operationalize the practices.</p> <p>This plan ensures:</p> <ul> <li>no overwhelming change</li> <li>gradual adoption where it makes the most sense</li> <li>leadership buy-in</li> <li>practitioner adoption</li> <li>predictable integration with current delivery processes</li> <li>early demonstration of value</li> <li>alignment with 3SF (Engagement \u2194 Delivery \u2194 Value)</li> </ul>"},{"location":"140-plan/#2-guiding-principles-for-adoption","title":"2. Guiding Principles for Adoption","text":"<ol> <li> <p>Do not replace existing estimation methods.    Align them.</p> </li> <li> <p>Start where pain is highest.    Complex, high-risk projects first.</p> </li> <li> <p>Multi-role participation is mandatory.    PM, PD, BA, Architect, EM, Lead Dev \u2014 all must score.</p> </li> <li> <p>Treat early adoption as learning.    Pilots, not audits.</p> </li> <li> <p>Transparency over precision.    The goal is predictability, not perfect numbers.</p> </li> <li> <p>No blame, no punishment.    Complexity drift is a signal, not a failure.</p> </li> <li> <p>Everything aligns to 3SF.    Engagement \u2192 Delivery \u2192 Value.</p> </li> </ol>"},{"location":"140-plan/#3-rollout-stages-end-to-end-roadmap","title":"3. Rollout Stages (End-to-End Roadmap)","text":"<p>The rollout consists of seven stages, each with clear outcomes and roles.</p>"},{"location":"140-plan/#stage-1-alignment-with-practice-leads-engineering-pmpdba-architecture","title":"Stage 1 \u2014 Alignment With Practice Leads (Engineering, PM/PD/BA, Architecture)","text":""},{"location":"140-plan/#goal","title":"Goal:","text":"<p>Gain agreement on the new model and prepare sponsors.</p>"},{"location":"140-plan/#activities","title":"Activities:","text":"<ul> <li>Present Pages 1\u20134 (Model \u2192 Translation Map \u2192 Scoring Sheet \u2192 Diagnostic)</li> <li>Discuss blind spots, pain points, and systemic gaps</li> <li>Confirm drivers, scales, and baseline structure</li> <li>Agree on pilot criteria</li> <li>Identify practice champions</li> </ul>"},{"location":"140-plan/#expected-outcome","title":"Expected Outcome:","text":"<p>Green light for organizational introduction + identified champions.</p>"},{"location":"140-plan/#stage-2-engineering-directors-review-approval","title":"Stage 2 \u2014 Engineering Directors Review &amp; Approval","text":""},{"location":"140-plan/#goal_1","title":"Goal:","text":"<p>Gain leadership commitment and agreement on expectations.</p>"},{"location":"140-plan/#activities_1","title":"Activities:","text":"<ul> <li>Present the initiative as alignment, not change</li> <li>Show how the diagnostic supports Directors (portfolio predictability)</li> <li>Confirm involvement expectations for PM, PD, BA, Architects</li> <li>Define rules for pilot setup</li> <li>Agree on reporting cadence</li> </ul>"},{"location":"140-plan/#expected-outcome_1","title":"Expected Outcome:","text":"<p>Directors endorse rollout and accept responsibility for supporting adoption.</p>"},{"location":"140-plan/#common-director-level-concerns-and-answers","title":"Common Director-Level Concerns (and answers):","text":"<ul> <li> <p>\u201cWill this slow us down?\u201d   \u2192 No. It reduces rework and escalations.</p> </li> <li> <p>\u201cDo we still use story points?\u201d   \u2192 Yes. They now align with complexity.</p> </li> <li> <p>\u201cDo we still use our RFP calculators?\u201d   \u2192 Yes. They now gain a complexity model.</p> </li> <li> <p>\u201cIs this extra overhead?\u201d   \u2192 Only upfront \u2014 but it removes downstream chaos.</p> </li> </ul>"},{"location":"140-plan/#stage-3-pilot-projects","title":"Stage 3 \u2014 Pilot Project(s)","text":""},{"location":"140-plan/#goal_2","title":"Goal:","text":"<p>Test and refine the model in a controlled environment.</p>"},{"location":"140-plan/#pilot-selection-criteria","title":"Pilot selection criteria:","text":"<p>Choose 1\u20132 projects that meet majority of these conditions:</p> <ul> <li>integration-heavy</li> <li>strong client involvement</li> <li>multiple roles (PM/PD/BA/Architect)</li> <li>medium or high uncertainty</li> <li>previous drift issues</li> <li>involved Practice Leads</li> <li>supportive client environment</li> </ul>"},{"location":"140-plan/#pilot-activities","title":"Pilot activities:","text":"<ul> <li>Apply baseline scoring during RFP or early discovery</li> <li>Track uncertainty burn-down</li> <li>Track complexity drift every 1\u20132 sprints</li> <li>Use translation map for forecasting</li> <li>Run the diagnostic at least twice</li> <li>Document findings &amp; improvements</li> </ul>"},{"location":"140-plan/#expected-outcome_2","title":"Expected Outcome:","text":"<p>Validated model + early wins + refined scoring sheets.</p>"},{"location":"140-plan/#stage-4-refinement-tool-finalization","title":"Stage 4 \u2014 Refinement &amp; Tool Finalization","text":""},{"location":"140-plan/#goal_3","title":"Goal:","text":"<p>Adapt the model based on real pilot experience.</p>"},{"location":"140-plan/#what-gets-refined","title":"What gets refined:","text":"<ul> <li>driver definitions</li> <li>scoring descriptions</li> <li>weightings (if any)</li> <li>ranges for multipliers</li> <li>SP mapping ranges</li> <li>diagnostic thresholds</li> <li>Excel sheet design</li> <li>reporting views for Directors</li> </ul>"},{"location":"140-plan/#expected-outcome_3","title":"Expected Outcome:","text":"<p>Version 2 of the scoring tool + clear guidance for generalized rollout.</p>"},{"location":"140-plan/#stage-5-gradual-rollout-to-high-risk-projects","title":"Stage 5 \u2014 Gradual Rollout to High-Risk Projects","text":""},{"location":"140-plan/#goal_4","title":"Goal:","text":"<p>Adopt model across projects where teams struggle the most.</p>"},{"location":"140-plan/#criteria-for-inclusion","title":"Criteria for inclusion:","text":"<ul> <li>complex domain</li> <li>heavy integrations</li> <li>multi-team dependencies</li> <li>large uncertainty</li> <li>unstable flows</li> <li>weak backlog health</li> </ul>"},{"location":"140-plan/#rollout-activities","title":"Rollout activities:","text":"<ul> <li>Onboarding session per project</li> <li>Initial baseline creation</li> <li>Complexity scoring in discovery</li> <li>Drift tracking every sprint</li> <li>Diagnostic every 4\u20136 weeks</li> <li>Directors review high-risk portfolios using diagnostic results</li> </ul>"},{"location":"140-plan/#expected-outcome_4","title":"Expected Outcome:","text":"<p>Lower escalation frequency + higher predictability in high-risk accounts.</p>"},{"location":"140-plan/#stage-6-rollout-to-standard-low-risk-projects","title":"Stage 6 \u2014 Rollout to Standard &amp; Low-Risk Projects","text":""},{"location":"140-plan/#goal_5","title":"Goal:","text":"<p>Normalize the model across all engagements.</p>"},{"location":"140-plan/#activities_2","title":"Activities:","text":"<ul> <li>Introduce light version of scoring</li> <li>Baseline for all new projects</li> <li>Use diagnostic monthly</li> <li>Integrate drivers into refinement rituals</li> <li>Include complexity/uncertainty in Definition of Ready policies</li> </ul>"},{"location":"140-plan/#expected-outcome_5","title":"Expected Outcome:","text":"<p>Organization-wide standardization of the estimation alignment model.</p>"},{"location":"140-plan/#stage-7-institutionalization","title":"Stage 7 \u2014 Institutionalization","text":""},{"location":"140-plan/#goal_6","title":"Goal:","text":"<p>Make this model part of the standard delivery system.</p>"},{"location":"140-plan/#activities_3","title":"Activities:","text":"<ul> <li>Add complexity &amp; uncertainty scoring into RFP workflow</li> <li>Add drift tracking to sprint reviews</li> <li>Add diagnostic to quarterly portfolio review</li> <li>Include driver scoring expectations in onboarding material</li> <li>Update best practice documentation</li> <li>Add 3SF alignment diagrams</li> <li>Integrate into project health dashboards</li> </ul>"},{"location":"140-plan/#expected-outcome_6","title":"Expected Outcome:","text":"<p>This model becomes:</p> <ul> <li>a standard practice</li> <li>a shared language</li> <li>a predictable way to forecast</li> <li>a foundation for client trust</li> <li>a key component of the 3SF execution layer</li> </ul>"},{"location":"140-plan/#4-roles-responsibilities-raci","title":"4. Roles &amp; Responsibilities (RACI)","text":"Role RFP Discovery Delivery Forecasting Value PM / PL A/R A/R R A/R A BA C R R C C PD / UX C R C C C Architect R R C C C Engineering Lead R C R R C Practice Leads A C C C C Engineering Directors A A A A A <p>Legend: R = Responsible, A = Accountable, C = Consulted.</p>"},{"location":"140-plan/#5-risks-mitigations","title":"5. Risks &amp; Mitigations","text":""},{"location":"140-plan/#risk-1-resistance-more-overhead","title":"Risk 1 \u2014 Resistance: \u201cMore overhead\u201d","text":"<p>Mitigation:</p> <ul> <li>Emphasize reduced chaos</li> <li>Show stage 3 pilot success stories</li> <li>Use light version on small projects</li> </ul>"},{"location":"140-plan/#risk-2-teams-skip-uncertainty-scoring","title":"Risk 2 \u2014 Teams skip uncertainty scoring","text":"<p>Mitigation:</p> <ul> <li>PM/BA/Architect jointly own it</li> <li>Include uncertainty in DoR</li> <li>Use diagnostic to flag stagnation</li> </ul>"},{"location":"140-plan/#risk-3-misuse-of-complexity-scores-eg-to-judge-performance","title":"Risk 3 \u2014 Misuse of complexity scores (e.g., to judge performance)","text":"<p>Mitigation:</p> <ul> <li>Enforce policy: complexity = signal, not KPI</li> <li>Reinforce message in every training</li> </ul>"},{"location":"140-plan/#risk-4-roles-dont-participate-eg-design-not-scoring","title":"Risk 4 \u2014 Roles don\u2019t participate (e.g., design not scoring)","text":"<p>Mitigation:</p> <ul> <li>Provide role-specific scoring guides</li> <li>Directors enforce expectations</li> </ul>"},{"location":"140-plan/#risk-5-clients-reject-new-approach","title":"Risk 5 \u2014 Clients reject new approach","text":"<p>Mitigation:</p> <ul> <li>Do not present complexity scoring externally</li> <li>Present outcomes, not mechanics</li> </ul>"},{"location":"140-plan/#6-communication-strategy-how-to-introduce-the-change","title":"6. Communication Strategy: How to Introduce the Change","text":""},{"location":"140-plan/#for-practice-leads","title":"For Practice Leads:","text":"<p>\u201cHere\u2019s how we eliminate estimation chaos.\u201d</p>"},{"location":"140-plan/#for-directors","title":"For Directors:","text":"<p>\u201cThis improves predictability and reduces escalations.\u201d</p>"},{"location":"140-plan/#for-pmpdba","title":"For PM/PD/BA:","text":"<p>\u201cThis clarifies your workload and protects you from unrealistic expectations.\u201d</p>"},{"location":"140-plan/#for-engineering-leads","title":"For Engineering Leads:","text":"<p>\u201cThis reduces pressure on velocity and shows real progress.\u201d</p>"},{"location":"140-plan/#for-teams","title":"For Teams:","text":"<p>\u201cThis gives us clarity and reduces surprises.\u201d</p>"},{"location":"140-plan/#for-clients-softly-presented","title":"For Clients (softly presented):","text":"<p>\u201cWe\u2019re improving internal alignment to give you more predictable delivery.\u201d</p>"},{"location":"140-plan/#7-rollout-timeline-example","title":"7. Rollout Timeline (Example)","text":"Month Actions 0\u20131 Practice Leads alignment, Directors approval 1\u20132 Pilot selection, training, baseline creation 2\u20134 Pilot operations, drift tracking, diagnostics 4\u20135 Tool refinement, process updates 5\u20138 High-risk project rollout 8\u201312 Low-risk rollout + institutionalization"},{"location":"140-plan/#8-success-criteria","title":"8. Success Criteria","text":""},{"location":"140-plan/#short-term","title":"Short-term:","text":"<ul> <li>complexity scoring used in pilots</li> <li>early drift detection</li> <li>fewer RFP-to-delivery surprises</li> <li>PM/PD/BA have defined workloads</li> <li>more stable planning</li> </ul>"},{"location":"140-plan/#medium-term","title":"Medium-term:","text":"<ul> <li>fewer escalations</li> <li>more predictable forecasting</li> <li>clearer trade-off decisions</li> <li>better client trust</li> </ul>"},{"location":"140-plan/#long-term","title":"Long-term:","text":"<ul> <li>estimation maturity across the org</li> <li>stable flows</li> <li>minimized rework</li> <li>improved profitability</li> <li>more predictable scaling</li> </ul> <p>All aligned to 3SF\u2019s engagement-delivery-value feedback loop.</p>"},{"location":"200-faq/","title":"Quick FAQ Review","text":"<ul> <li>How This Model Fits With Existing RFP and Fixed-Bid Estimation Processes?     &gt; Discovery estimates complexity &amp; uncertainty, Delivery estimates slices &amp; story points, and PM/PL keep both tracks aligned by maintaining a shared Complexity Baseline, a readiness buffer, and weekly track syncs.     &gt; This makes Dual-Track predictable, measurable, and aligned end-to-end.</li> <li>How Do We Prevent Story Point Inflation or Re-Estimation Games?     &gt; We prevent story point inflation by anchoring SP to complexity ranges, scoring complexity separately, updating the baseline when drift occurs, and using SP only for slicing and flow \u2014 never for forecasting, cost, or performance.     &gt; This restores honesty, stability, and predictability.</li> <li>Who Is Responsible for Scoring Complexity &amp; Uncertainty \u2014 and How Often Do We Update the Baseline?     &gt; Complexity &amp; uncertainty scoring is a multi-role activity:     &gt; PM, BA, PD, Architect, and EM each score the parts they understand best.     &gt; The baseline is created during RFP (v1), updated after Discovery (v2), and only updated during Delivery when real complexity drift occurs (v3).     &gt; Drift tracking is continuous \u2014 baseline updates are intentional.</li> <li>How Do We Handle Complexity Drift Without Triggering Client Escalations or Immediate Renegotiation?     &gt; We handle complexity drift through early detection, internal alignment, structured client conversations, and fact-based trade-offs \u2014 not by triggering immediate renegotiation.     &gt; Complexity drift is normal; surprises are not.</li> <li>How Does This Model Scale in Multi-Team or Multi-Vendor Environments?     &gt; The model scales by giving all teams and vendors a shared set of complexity &amp; uncertainty drivers, a unified baseline, a shared dependency map, and a program-level drift dashboard.     &gt; Each team estimates independently, but the system aligns at the program level \u2014 creating predictable multi-team delivery.</li> <li>How Estimation Aligns Across Dual-Track Scrum (Discovery + Delivery)?     &gt; Discovery estimates complexity &amp; uncertainty, Delivery estimates slices &amp; story points, and PM/PL keep both tracks aligned by maintaining a shared Complexity Baseline, a readiness buffer, and weekly track syncs.     &gt; This makes Dual-Track predictable, measurable, and aligned end-to-end.</li> <li>Does Tracking Complexity and Uncertainty Increase Overhead? How Do We Keep It Lightweight?     &gt; No \u2014 tracking complexity and uncertainty does not add overhead.     &gt; It replaces invisible, high-cost overhead (rework, re-estimation, surprises) with a fast, lightweight alignment tool.     &gt; Teams score only meaningful features, only when needed, using a simple scale \u2014 and save significant effort later in delivery.</li> <li>How Does This Model Help Us Make Scope, Sequencing, and Value-Tradeoff Decisions With Clients?     &gt; The model enables clear, evidence-based decisions about scope, sequencing, and value by making complexity, uncertainty, and drift visible.     &gt; It turns trade-off discussions into structured, data-driven conversations rather than subjective debates \u2014 and helps clients make informed decisions.</li> <li>What Happens When New Scope Appears Mid-Project \u2014 How Do We Score, Track, and Communicate It?     &gt; When new scope appears, we score it using the same complexity and uncertainty drivers, compare it against the baseline, and present clear trade-off options to the client.     &gt; This keeps delivery stable, protects the team, and turns scope changes into professional, data-driven decisions \u2014 not disruption.</li> <li>How Do We Handle Contradictions Between Estimation Methods \u2014 FTEs, Story Points, Flow Metrics, and Forecasts?     &gt; FTE, story points, velocity, cycle time, and throughput appear to contradict one another because they measure different things.     &gt; The Complexity &amp; Uncertainty Model acts as the unifying foundation that connects them into a single consistent system.     &gt; Once complexity is scored and baselined, every estimation method aligns naturally \u2014 and contradictions become meaningful signals, not points of conflict.</li> <li>When and How to Use Common Estimation Methods (T-Shirt, Story Points, PERT/Statistical PERT, etc.) \u2014 And How They Align Through the Model?     &gt; Each estimation method serves a different purpose in the lifecycle \u2014 from coarse sizing (T-Shirt) to slicing (Story Points) to risk modeling (PERT) to flow prediction (Throughput &amp; Cycle Time).     &gt; The Complexity &amp; Uncertainty Model aligns all of them by providing a shared foundation.     &gt; This turns conflicting methods into a coherent, unified estimation system.</li> </ul>"},{"location":"201-faq/","title":"How This Model Fits With Existing RFP and Fixed-Bid Estimation Processes?","text":"<p>Our RFP process stays the same. What changes is that estimates now have a structured complexity &amp; uncertainty model behind them, making cost, scope, and timelines more predictable and transparent \u2014 and giving Delivery the context it needs to succeed.</p> <p>How to align cost estimation, complexity scoring, and delivery predictability without replacing current methods.</p>"},{"location":"201-faq/#1-why-this-question-matters","title":"1. Why This Question Matters","text":"<p>Every organization already has a way to answer the big pre-sales questions:</p> <ul> <li>\u201cHow much will it cost?\u201d</li> <li>\u201cHow many people do we need?\u201d</li> <li>\u201cHow long will it take?\u201d</li> </ul> <p>These are answered through:</p> <ul> <li>FTE-based effort modeling</li> <li>sprint counts</li> <li>blended rates</li> <li>role allocations</li> <li>overhead and risk modifiers</li> </ul> <p>The concern is: Does the complexity &amp; uncertainty model change these workflows? Does it replace them? Does it require new tools?</p> <p>The short answer: No \u2014 it strengthens them. It makes them more honest, traceable, and predictable.</p> <p>This page explains how.</p>"},{"location":"201-faq/#2-what-stays-the-same","title":"2. What Stays the Same","text":"<p>Your existing RFP estimation workflow does not disappear.</p> <p>Teams still:</p> <ul> <li>create EPIC / capability lists</li> <li>estimate effort per component</li> <li>calculate needed roles</li> <li>compute team cost</li> <li>apply risk buffers</li> <li>produce a price</li> <li>respond to RFPs on time</li> </ul> <p>The organization does not change:</p> <ul> <li>pricing structure</li> <li>blended rates</li> <li>margin models</li> <li>sprint cost calculations</li> <li>role mix assumptions</li> <li>sales processes</li> </ul> <p>All existing mechanics remain fully compatible.</p>"},{"location":"201-faq/#3-what-changes-and-why-it-matters","title":"3. What Changes (and Why It Matters)","text":""},{"location":"201-faq/#a-complexity-becomes-the-foundation-for-effort-estimation","title":"A. Complexity becomes the foundation for effort estimation","text":"<p>Today, FTE effort is mostly based on:</p> <ul> <li>prior experience</li> <li>intuition</li> <li>heuristic judgments</li> <li>\u201cthis feels like 2 back-end devs for 10 sprints\u201d</li> </ul> <p>The new model introduces scored complexity drivers that make these assumptions explicit.</p> <p>This improves:</p> <ul> <li>transparency</li> <li>accuracy</li> <li>confidence</li> <li>repeatability</li> <li>consistency across teams</li> </ul> <p>It also reduces the risk of:</p> <ul> <li>underestimating integrations</li> <li>ignoring dependencies</li> <li>forgetting discovery/design complexity</li> <li>misjudging non-functional load</li> <li>overconfident timelines</li> </ul>"},{"location":"201-faq/#b-uncertainty-becomes-a-first-class-input","title":"B. Uncertainty becomes a first-class input","text":"<p>Instead of applying a flat \u201crisk modifier,\u201d we now score uncertainty using explicit factors such as:</p> <ul> <li>unclear requirements</li> <li>missing documentation</li> <li>unknown dependencies</li> <li>unstable client ownership</li> <li>unclear UX direction</li> <li>regulatory ambiguity</li> </ul> <p>This creates:</p> <ul> <li>better risk modeling</li> <li>better expectation setting</li> <li>more realistic buffers</li> <li>clear justification for contingency</li> </ul>"},{"location":"201-faq/#c-rfp-estimates-become-anchored-to-a-complexity-baseline","title":"C. RFP estimates become anchored to a Complexity Baseline","text":"<p>The RFP estimate ceases to be a black box and becomes a traceable set of assumptions.</p> <p>This baseline allows Delivery teams to:</p> <ul> <li>understand what was assumed</li> <li>understand where drift begins</li> <li>detect scope expansion early</li> <li>anchor story points to original expectations</li> <li>justify re-estimation</li> <li>protect the timeline and budget</li> </ul> <p>Without a baseline, Delivery inherits a number without context. With a baseline, they inherit a model.</p>"},{"location":"201-faq/#4-how-the-model-fits-into-the-rfp-workflow-step-by-step","title":"4. How the Model Fits Into the RFP Workflow (Step-by-Step)","text":"<p>Below is the exact flow, showing how little the workflow changes and how much clarity improves.</p>"},{"location":"201-faq/#step-1-build-epiccapability-list-same-as-today","title":"\u2b50 Step 1 \u2014 Build EPIC/Capability List (Same as today)","text":"<p>No changes. This remains the starting point.</p>"},{"location":"201-faq/#step-2-score-complexity-per-epic-new-clarity-layer","title":"\u2b50 Step 2 \u2014 Score Complexity per EPIC (New clarity layer)","text":"<p>Each high-level item receives complexity scoring using 8\u201310 drivers.</p> <p>This:</p> <ul> <li>exposes hidden effort</li> <li>identifies integration risks</li> <li>produces comparable estimates across PM/PD/BA/Architect/Dev</li> <li>reduces subjective anchoring</li> </ul>"},{"location":"201-faq/#step-3-score-uncertainty-per-epic-new-risk-layer","title":"\u2b50 Step 3 \u2014 Score Uncertainty per EPIC (New risk layer)","text":"<p>This provides nuance to risk-based adjustments.</p> <p>Instead of a generic \u201c20\u201340% buffer,\u201d teams apply uncertainty multipliers grounded in reality.</p>"},{"location":"201-faq/#step-4-convert-complexity-fte-effort-same-as-today","title":"\u2b50 Step 4 \u2014 Convert Complexity \u2192 FTE Effort (Same as today)","text":"<p>The model uses your existing cost calculator with improved inputs.</p> <p>Example: \u201cBase effort = 2 dev-weeks, complexity = high \u2192 multiplier \u00d71.5\u201d</p> <p>This is familiar to teams \u2014 no new tool required.</p>"},{"location":"201-faq/#step-5-apply-uncertainty-multiplier-more-accurate-buffers","title":"\u2b50 Step 5 \u2014 Apply Uncertainty Multiplier (More accurate buffers)","text":"<p>Uncertainty becomes the driver for risk buffers, not a gut feel.</p> <p>High uncertainty \u2192 wider cost/timeline range Low uncertainty \u2192 tighter range</p>"},{"location":"201-faq/#step-6-build-team-composition-same-as-today-but-justified","title":"\u2b50 Step 6 \u2014 Build Team Composition (Same as today, but justified)","text":"<p>PM, BA, PD, Architect effort is no longer guessed \u2014 it is complexity-driven.</p>"},{"location":"201-faq/#step-7-produce-rfp-cost-timeline-same-deliverables-as-before","title":"\u2b50 Step 7 \u2014 Produce RFP Cost &amp; Timeline (Same deliverables as before)","text":"<p>Nothing changes in how we prepare the final RFP package:</p> <ul> <li>scope</li> <li>assumptions</li> <li>team</li> <li>cost</li> <li>timeline</li> <li>risks</li> </ul> <p>What changes is confidence and alignment.</p>"},{"location":"201-faq/#5-how-this-helps-delivery-teams-the-biggest-improvement","title":"5. How This Helps Delivery Teams (The Biggest Improvement)","text":"<p>Delivery no longer receives:</p> <ul> <li>a number</li> <li>a list of EPICs</li> <li>a loosely defined timeline</li> <li>a scope spreadsheet</li> </ul> <p>They now receive:</p> <ul> <li>Complexity Baseline</li> <li>Uncertainty Baseline</li> <li>assumptions per EPIC</li> <li>dependency risks</li> <li>integration challenges</li> <li>scoring rationale</li> </ul> <p>This solves the #1 source of tension in most organizations:</p> <p>\u201cRFP gave us an unrealistic estimate \u2014 now delivery must make it work.\u201d</p> <p>With a shared model, everyone sees:</p> <ul> <li>where assumptions were strong</li> <li>where assumptions were weak</li> <li>where drift happens</li> <li>where renegotiation is justified</li> <li>where scope must be sliced</li> <li>where capacity must be adjusted</li> </ul> <p>This creates transparency and strengthens trust across 3SF lines:</p> <ul> <li>Engagement (client \u2194 vendor)</li> <li>Delivery (vendor \u2194 product)</li> <li>Value (product \u2194 client)</li> </ul>"},{"location":"202-faq/","title":"How Do We Prevent Story Point Inflation or Re-Estimation Games?","text":"<p>We prevent story point inflation by anchoring SP to complexity ranges, scoring complexity separately, updating the baseline when drift occurs, and using SP only for slicing and flow \u2014 never for forecasting, cost, or performance. This restores honesty, stability, and predictability.</p> <p>Why story points drift, and how the Complexity &amp; Uncertainty Model restores honesty and stability.</p>"},{"location":"202-faq/#1-why-this-question-exists","title":"1. Why This Question Exists","text":"<p>Teams often see story points inflate, shift, or drift over time. Common symptoms:</p> <ul> <li>\u201cEverything became a 5\u2026 then an 8\u2026 then a 13\u2026\u201d</li> <li>\u201cVelocity went up, but timelines still slipped.\u201d</li> <li>\u201cWe re-estimated all stories and magically created \u2018more velocity.\u2019\u201d</li> <li>\u201cStory points no longer correlate to actual complexity.\u201d</li> <li>\u201cProduct thinks we\u2019re slow; Engineering thinks the estimation was wrong.\u201d</li> </ul> <p>The underlying issue: Story points are being used for the wrong purpose.</p> <p>Teams use them as:</p> <ul> <li>effort estimates</li> <li>deadlines</li> <li>KPIs</li> <li>velocity boosters</li> <li>negotiation tools</li> </ul> <p>All of these uses contaminate the system.</p> <p>This page explains how the Complexity &amp; Uncertainty Model fixes it.</p>"},{"location":"202-faq/#2-what-story-points-are-and-are-not","title":"2. What Story Points Are \u2014 and Are Not","text":""},{"location":"202-faq/#story-points-are","title":"Story Points are:","text":"<ul> <li>a tool for relative sizing</li> <li>a way for the delivery team to compare slices of work</li> <li>a mechanism to help forecast throughput</li> <li>a tactical unit for planning sprints</li> </ul>"},{"location":"202-faq/#story-points-are-not","title":"Story Points are not:","text":"<ul> <li>a cost estimate</li> <li>a measure of value</li> <li>a measure of performance</li> <li>a commitment</li> <li>a delivery target</li> <li>a proxy for complexity</li> <li>a tool for \u201chow long will this take\u201d</li> </ul> <p>This is where inflation starts: SP are pulled into a role they were never meant to fill.</p>"},{"location":"202-faq/#3-why-story-points-drift-the-real-causes","title":"3. Why Story Points Drift (The Real Causes)","text":""},{"location":"202-faq/#cause-1-no-baseline-complexity-model","title":"Cause 1 \u2014 No baseline complexity model","text":"<p>Teams re-size stories without knowing what the RFP assumed.</p>"},{"location":"202-faq/#cause-2-uncertainty-is-not-quantified","title":"Cause 2 \u2014 Uncertainty is not quantified","text":"<p>Teams inflate points to account for unknowns.</p>"},{"location":"202-faq/#cause-3-sp-attached-to-deadlines","title":"Cause 3 \u2014 SP attached to deadlines","text":"<p>Teams make points bigger to look faster.</p>"},{"location":"202-faq/#cause-4-no-complexity-range-mapping","title":"Cause 4 \u2014 No complexity range mapping","text":"<p>Simplicity becomes subjective \u2192 inflation grows.</p>"},{"location":"202-faq/#cause-5-missing-drift-tracking","title":"Cause 5 \u2014 Missing drift tracking","text":"<p>Delivery cannot detect when complexity grows, so they compensate by re-estimating.</p>"},{"location":"202-faq/#cause-6-sp-used-for-performance-management","title":"Cause 6 \u2014 SP used for performance management","text":"<p>Teams become defensive; points become political.</p>"},{"location":"202-faq/#4-how-the-complexity-model-fixes-point-inflation","title":"4. How the Complexity Model Fixes Point Inflation","text":"<p>The key is simple:</p> <p>Story points no longer describe complexity. Complexity describes complexity.</p> <p>SP return to their intended purpose: estimating slices of known work, not interpreting RFP assumptions.</p> <p>Here\u2019s the structural fix.</p>"},{"location":"202-faq/#fix-1-story-points-map-to-complexity-ranges","title":"\u2b50 Fix 1 \u2014 Story points map to complexity ranges","text":"<p>Each complexity level has a bounded SP range:</p> Complexity Level SP Range Low (1\u20133) 1\u20133 Medium (4\u20137) 5\u20138 High (8\u201312) 13\u201321 Extreme (13+) &gt;21 or split <p>This removes:</p> <ul> <li>runaway story point escalation</li> <li>re-estimation inflation</li> <li>subjective inflation</li> <li>\u201cthis feels big so let\u2019s make it a 13\u201d behavior</li> </ul> <p>Now SP have meaningful constraints.</p>"},{"location":"202-faq/#fix-2-re-estimation-cannot-overwrite-the-baseline","title":"\u2b50 Fix 2 \u2014 Re-estimation cannot overwrite the baseline","text":"<p>Delivery cannot simply \u201cmake stories bigger.\u201d They must classify:</p> <ul> <li>new complexity \u2192 complexity drift</li> <li>uncertainty discovered \u2192 uncertainty drift</li> </ul> <p>This builds honesty into the system.</p>"},{"location":"202-faq/#fix-3-sp-measure-delivery-slicing-not-rfp-assumptions","title":"\u2b50 Fix 3 \u2014 SP measure delivery slicing, not RFP assumptions","text":"<p>RFP complexity is scored. Delivery complexity is sliced.</p> <p>SP only measure:</p> <ul> <li>how thinly</li> <li>how appropriately</li> <li>how predictably</li> <li>the team slices a feature into implementable increments</li> </ul> <p>Not:</p> <ul> <li>scope</li> <li>unknowns</li> <li>architecture load</li> <li>integration risk</li> </ul> <p>These belong to the complexity baseline.</p>"},{"location":"202-faq/#fix-4-velocity-is-interpreted-correctly","title":"\u2b50 Fix 4 \u2014 Velocity is interpreted correctly","text":"<p>With complexity tracked separately:</p> <ul> <li>velocity becomes a capacity signal</li> <li>velocity no longer becomes a pressure valve</li> <li>velocity stops being gamed</li> <li>velocity stays stable over time</li> <li>velocity becomes comparable across sprints</li> </ul> <p>Delivery regains trust in its own metrics.</p>"},{"location":"202-faq/#fix-5-uncertainty-is-handled-with-spikes-not-point-inflation","title":"\u2b50 Fix 5 \u2014 Uncertainty is handled with spikes, not point inflation","text":"<p>Unknowns trigger:</p> <ul> <li>spikes</li> <li>clarification sessions</li> <li>PoCs</li> </ul> <p>Not:</p> <ul> <li>\u201cmake it a 13 just in case\u201d</li> </ul> <p>This prevents SP from absorbing risk incorrectly.</p>"},{"location":"202-faq/#5-how-pm-and-pl-enforce-the-fix","title":"5. How PM and PL Enforce the Fix","text":""},{"location":"202-faq/#pm-responsibilities","title":"PM responsibilities:","text":"<ul> <li>ensure every EPIC has a complexity score</li> <li>ensure uncertainty is scored and tracked</li> <li>identify new complexity early</li> <li>align with PL in weekly track sync</li> </ul>"},{"location":"202-faq/#pl-em-responsibilities","title":"PL / EM responsibilities:","text":"<ul> <li>ensure SP within the allowed range for complexity level</li> <li>prevent re-estimation from masking drift</li> <li>ensure SP reflect slices, not assumptions</li> <li>ensure velocity remains a capacity indicator</li> </ul>"},{"location":"202-faq/#shared-responsibilities","title":"Shared responsibilities:","text":"<ul> <li>maintain Complexity Baseline</li> <li>communicate drift</li> <li>align forecast expectations</li> </ul>"},{"location":"202-faq/#6-what-this-looks-like-in-real-scenarios","title":"6. What This Looks Like in Real Scenarios","text":""},{"location":"202-faq/#before","title":"Before:","text":"<p>Integration becomes more complex \u2192 team re-estimates from 8 \u2192 13 \u2192 21 \u2192 34 points.</p> <p>Velocity appears to rise. Timeline still slips. Trust erodes.</p>"},{"location":"202-faq/#after","title":"After:","text":"<p>Integration complexity increases \u2192 team rescoring shows +7 drift \u2192 PM/PL renegotiate scope and forecast \u2192 velocity stays clean \u2192 stakeholders understand why timeline moves.</p> <p>Trust increases.</p>"},{"location":"203-faq/","title":"Who Is Responsible for Scoring Complexity &amp; Uncertainty \u2014 and How Often Do We Update the Baseline?","text":"<p>Complexity &amp; uncertainty scoring is a multi-role activity: PM, BA, PD, Architect, and EM each score the parts they understand best. The baseline is created during RFP (v1), updated after Discovery (v2), and only updated during Delivery when real complexity drift occurs (v3). Drift tracking is continuous \u2014 baseline updates are intentional.</p> <p>Clarifying ownership, timing, and collaboration across roles.</p>"},{"location":"203-faq/#1-why-this-question-exists","title":"1. Why This Question Exists","text":"<p>Every organization hits the same three pain points:</p> <ol> <li> <p>No one is sure who should score what    PM? BA? PD? Architect? Devs? EM? PL? All? None?</p> </li> <li> <p>Scoring happens at the wrong times    Too early \u2192 inaccurate    Too late \u2192 chaotic    Repeatedly \u2192 wasteful    Never \u2192 dangerous</p> </li> <li> <p>Baselines are created once and then forgotten    Delivery inherits a \u201cfixed estimate\u201d with no way to evaluate how assumptions changed.</p> </li> </ol> <p>This page explains exactly who owns scoring, who contributes, and how frequently the baseline should be updated.</p>"},{"location":"203-faq/#2-core-principle-scoring-is-cross-functional-ownership-is-shared","title":"2. Core Principle: Scoring Is Cross-Functional, Ownership Is Shared","text":"<p>Complexity &amp; uncertainty cannot be scored by a single role, because each role has visibility into different dimensions:</p> <ul> <li>BA \u2192 requirement clarity</li> <li>PD \u2192 UX &amp; workflow complexity</li> <li>Architect \u2192 integration, NFR, data, domain</li> <li>PM \u2192 stakeholder alignment, dependency risk</li> <li>EM/Lead \u2192 implementation complexity</li> <li>Devs \u2192 slicing complexity, technical nuance</li> </ul> <p>This is fundamentally a multi-role activity.</p> <p>But ownership must be clear.</p>"},{"location":"203-faq/#3-ownership-model-raci","title":"3. Ownership Model (RACI)","text":"Activity PM BA PD Architect EM/PL Dev Team Identify EPIC/Feature list A/R C C C C \u2014 Score Complexity A R R R R C Score Uncertainty A R R R C \u2014 Create Baseline v1 (RFP) A/R C C R C \u2014 Create Baseline v2 (Post-Discovery) A/R R R R C C Track Drift A/R C C C A/R C Trigger Baseline Update A \u2014 \u2014 R R \u2014 <p>Legend: A = Accountable, R = Responsible, C = Consulted</p> <p>This matrix keeps responsibility distributed without creating ambiguity.</p>"},{"location":"203-faq/#4-what-each-role-scores-breakdown","title":"4. What Each Role Scores (Breakdown)","text":""},{"location":"203-faq/#pm-projectprogram-manager","title":"PM (Project/Program Manager)","text":"<p>Scores:</p> <ul> <li>requirement clarity</li> <li>stakeholder alignment</li> <li>delivery constraints</li> <li>external dependencies</li> <li>operational/process uncertainty</li> </ul> <p>Why: PMs manage alignment and risk visibility.</p>"},{"location":"203-faq/#ba-business-analyst","title":"BA (Business Analyst)","text":"<p>Scores:</p> <ul> <li>requirement depth</li> <li>ambiguous logic</li> <li>gaps in acceptance criteria</li> <li>documentation quality</li> </ul> <p>Why: BAs understand functional truth and its vagueness.</p>"},{"location":"203-faq/#pd-product-designer-ux","title":"PD (Product Designer / UX)","text":"<p>Scores:</p> <ul> <li>UI/UX complexity</li> <li>workflow branching</li> <li>user interactions</li> <li>design uncertainty</li> </ul> <p>Why: UX complexity is often the hidden multiplier in delivery effort.</p>"},{"location":"203-faq/#architect","title":"Architect","text":"<p>Scores:</p> <ul> <li>integration complexity</li> <li>data complexity</li> <li>non-functional requirements</li> <li>platform constraints</li> <li>technical unknowns</li> </ul> <p>Why: architecture drives the upper bound of engineering effort.</p>"},{"location":"203-faq/#empl-engineering-manager-project-lead","title":"EM/PL (Engineering Manager / Project Lead)","text":"<p>Scores:</p> <ul> <li>implementation complexity</li> <li>technical dependencies</li> <li>slicing feasibility</li> <li>testability complexity</li> </ul> <p>Why: EM/PLs own the delivery execution.</p>"},{"location":"203-faq/#developers","title":"Developers","text":"<p>Contribute to:</p> <ul> <li>technical nuance</li> <li>incremental complexity</li> <li>edge-case awareness</li> <li>splitting stories</li> </ul> <p>Why: devs are closest to the code and can detect hidden complexity early.</p>"},{"location":"203-faq/#5-when-to-score-frequency","title":"5. When To Score (Frequency)","text":"<p>The baseline is not static. It evolves as clarity evolves.</p> <p>Below is the recommended update rhythm.</p>"},{"location":"203-faq/#baseline-v1-rfp-stage-initial-assumptions","title":"\u2b50 Baseline v1 \u2014 RFP Stage (Initial Assumptions)","text":"<p>When:</p> <ul> <li>first estimate</li> <li>early capability breakdown</li> </ul> <p>Who:</p> <ul> <li>PM + Architect + BA + PD + EM</li> </ul> <p>Purpose:</p> <ul> <li>guide costing</li> <li>estimate timeline</li> <li>inform staffing</li> <li>establish assumptions</li> </ul>"},{"location":"203-faq/#baseline-v2-after-discovery-design-clarity-pass","title":"\u2b50 Baseline v2 \u2014 After Discovery &amp; Design (Clarity Pass)","text":"<p>When:</p> <ul> <li>after requirements clarified</li> <li>after UX flows drafted</li> <li>after architecture is mapped</li> <li>after key dependencies identified</li> </ul> <p>Who:</p> <ul> <li>PM + BA + PD + Architect</li> </ul> <p>Purpose:</p> <ul> <li>convert assumptions \u2192 validated complexity</li> <li>reduce uncertainty</li> <li>enable stable sprint planning</li> <li>adjust forecasts before coding starts</li> </ul> <p>This is the most important baseline version.</p>"},{"location":"203-faq/#baseline-v3-during-delivery-drift-detection","title":"\u2b50 Baseline v3 \u2014 During Delivery (Drift Detection)","text":"<p>When:</p> <ul> <li>complexity drift crosses threshold (e.g., 15\u201325%)</li> <li>major new scope appears</li> <li>new integration constraints discovered</li> <li>dependencies change</li> <li>uncertainty spikes</li> </ul> <p>Who:</p> <ul> <li>PM + Architect + EM/PL</li> </ul> <p>Purpose:</p> <ul> <li>stay ahead of drift</li> <li>adjust timeline transparently</li> <li>prevent velocity pressure</li> <li>justify re-estimation</li> <li>protect team and client expectations</li> </ul> <p>Baseline v3 is not automatic \u2014 it\u2019s triggered by real events.</p>"},{"location":"203-faq/#ongoing-drift-tracking-every-sprint","title":"\u2b50 Ongoing Drift Tracking \u2014 Every Sprint","text":"<p>Frequency:</p> <ul> <li>Complexity rescored when new info arrives</li> <li>Uncertainty reviewed weekly</li> <li>Drift trend reviewed in sprint reviews or PM/PL sync</li> </ul> <p>Purpose:</p> <ul> <li>detect misalignment early</li> <li>prevent \u201cslow deterioration\u201d</li> <li>avoid late surprises</li> <li>create predictable flow</li> </ul>"},{"location":"203-faq/#6-when-not-to-update-the-baseline","title":"6. When NOT To Update the Baseline","text":"<p>The baseline should not be updated:</p> <ul> <li>to \u201cfix\u201d velocity</li> <li>to make SP look right</li> <li>to hide complexity</li> <li>to force artificial alignment</li> <li>because someone feels uncomfortable</li> <li>because leadership wants a smoother forecast</li> </ul> <p>If none of the drivers changed \u2014 the baseline must remain stable.</p> <p>This maintains the integrity of drift signals.</p>"},{"location":"203-faq/#7-how-this-prevents-estimation-chaos","title":"7. How This Prevents Estimation Chaos","text":"<p>This model ensures:</p>"},{"location":"203-faq/#no-single-role-is-overburdened","title":"No single role is overburdened","text":"<p>Complexity is scored by experts in each domain.</p>"},{"location":"203-faq/#delivery-inherits-a-realistic-context","title":"Delivery inherits a realistic context","text":"<p>Baseline makes assumptions visible.</p>"},{"location":"203-faq/#uncertainty-decreases-predictably","title":"Uncertainty decreases predictably","text":"<p>Burn-down reveals discovery gaps.</p>"},{"location":"203-faq/#drift-becomes-measurable","title":"Drift becomes measurable","text":"<p>Teams no longer rely on \u201cit feels harder.\u201d</p>"},{"location":"203-faq/#re-estimation-becomes-justified","title":"Re-estimation becomes justified","text":"<p>Because drift is visible, not emotional.</p>"},{"location":"203-faq/#velocity-becomes-clean","title":"Velocity becomes clean","text":"<p>No point inflation, no gaming.</p>"},{"location":"203-faq/#stakeholder-conversations-improve","title":"Stakeholder conversations improve","text":"<p>Discussion becomes factual, not political.</p>"},{"location":"204-faq/","title":"How Do We Handle Complexity Drift Without Triggering Client Escalations or Immediate Renegotiation?","text":"<p>We handle complexity drift through early detection, internal alignment, structured client conversations, and fact-based trade-offs \u2014 not by triggering immediate renegotiation. Complexity drift is normal; surprises are not.</p> <p>How to manage scope shifts with transparency, professionalism, and predictability \u2014 while maintaining trust.</p>"},{"location":"204-faq/#1-why-this-question-matters","title":"1. Why This Question Matters","text":"<p>Every project experiences complexity drift:</p> <ul> <li>integrations are harder than expected</li> <li>dependencies move slower</li> <li>unclear requirements expand</li> <li>UX flows grow</li> <li>data model complexity reveals itself</li> <li>external teams are not ready</li> <li>hidden constraints emerge</li> </ul> <p>But the fear is real:</p> <ul> <li>\u201cIf we admit complexity increased, the client will panic.\u201d</li> <li>\u201cWe can\u2019t renegotiate every time something changes.\u201d</li> <li>\u201cIf we escalate every drift, we will lose goodwill.\u201d</li> <li>\u201cDelivery feels pressure to hide drift to avoid conflict.\u201d</li> </ul> <p>This page explains how to manage drift without causing unnecessary escalation \u2014 and how to keep trust intact.</p>"},{"location":"204-faq/#2-the-core-principle-drift-is-normal-surprises-are-not","title":"2. The Core Principle: Drift Is Normal \u2014 Surprises Are Not","text":"<p>Complexity drift is not a failure. It is the natural outcome of discovery.</p> <p>The breakdown happens when:</p> <ul> <li>drift is discovered late</li> <li>drift is communicated emotionally</li> <li>drift is not quantified</li> <li>drift is not structured</li> <li>drift is confused with failure</li> <li>drift is used as a tool for blame</li> </ul> <p>The goal is early, structured, low-friction transparency \u2014 not panic escalation.</p>"},{"location":"204-faq/#3-what-causes-escalations-in-the-first-place","title":"3. What Causes Escalations in the First Place?","text":"<p>Not drift itself \u2014 but the lack of alignment around drift.</p> <p>Top causes of escalation:</p> <ol> <li>Surprises</li> <li>Emotional communication</li> <li>No baseline to compare against</li> <li>Blaming teams instead of assumptions</li> <li>Vague explanations (\u201cit was harder than expected\u201d)</li> <li>Late-stage revelations when options are gone</li> <li>Pressure on velocity instead of honest drift reporting</li> </ol> <p>The model solves all seven.</p>"},{"location":"204-faq/#4-how-the-complexity-model-removes-escalation-pressure","title":"4. How the Complexity Model Removes Escalation Pressure","text":""},{"location":"204-faq/#a-baseline-makes-assumptions-visible","title":"A. Baseline makes assumptions visible","text":"<p>So drift is a gap between assumptions and reality \u2014 not a team failure.</p>"},{"location":"204-faq/#b-drift-tracking-quantifies-change","title":"B. Drift tracking quantifies change","text":"<p>No more subjective \u201cit feels harder.\u201d</p>"},{"location":"204-faq/#c-drivers-explain-why-complexity-changed","title":"C. Drivers explain why complexity changed","text":"<p>Example: \u201cIntegration complexity +3 due to undocumented API changes.\u201d</p>"},{"location":"204-faq/#d-uncertainty-burn-down-shows-what-was-known-vs-unknown","title":"D. Uncertainty burn-down shows what was known vs unknown","text":"<p>We can say: \u201cThis was in the yellow zone from the start \u2014 here\u2019s where clarity increased.\u201d</p>"},{"location":"204-faq/#e-early-detection-early-trade-offs","title":"E. Early detection \u2192 early trade-offs","text":"<p>Small adjustments early prevent big renegotiations later.</p>"},{"location":"204-faq/#5-the-three-stage-drift-management-strategy","title":"5. The Three-Stage Drift Management Strategy","text":"<p>This is the exact method used by mature product consultancies to avoid conflict and strengthen trust.</p>"},{"location":"204-faq/#stage-1-internal-alignment-pm-pl-architect-ba-pd","title":"\u2b50 Stage 1 \u2014 Internal Alignment (PM + PL + Architect + BA + PD)","text":"<p>Frequency:</p> <ul> <li>weekly</li> <li>whenever drift &gt; 10%</li> <li>or when major new complexity appears</li> </ul> <p>Steps:</p> <ol> <li>Identify drift driver (integration, UX, data, dependencies, etc.)</li> <li>Quantify drift (e.g., +6 complexity, +18% total)</li> <li>Understand impact (timeline, capacity, dependencies)</li> <li>Decide whether drift requires baseline update</li> <li>Prepare a simple narrative for stakeholders</li> </ol> <p>Outcome: PM and PL align before speaking to client \u2192 no panic, no confusion.</p>"},{"location":"204-faq/#stage-2-low-friction-client-alignment-no-renegotiation-yet","title":"\u2b50 Stage 2 \u2014 Low-Friction Client Alignment (No Renegotiation Yet)","text":"<p>This stage aims to inform, not negotiate.</p> <p>PM communicates:</p> <ul> <li>what changed</li> <li>what impact is expected</li> <li>what options exist</li> <li>where uncertainty decreased</li> <li>what will be rechecked in next iteration</li> </ul> <p>The tone is:</p> <ul> <li>calm</li> <li>factual</li> <li>non-blaming</li> <li>structured</li> </ul> <p>This is typically done via:</p> <ul> <li>weekly Steering</li> <li>roadmap session</li> <li>dependency review</li> </ul> <p>This stage prevents escalation because the client is prepared before it becomes a problem.</p>"},{"location":"204-faq/#stage-3-structured-trade-off-conversation-only-if-needed","title":"\u2b50 Stage 3 \u2014 Structured Trade-Off Conversation (Only If Needed)","text":"<p>Only after multiple drift signals accumulate (e.g., &gt;25\u201330%), PM initiates a structured conversation:</p> <ol> <li>Present drift trend (not a one-time surprise)</li> <li>Show baseline vs actual (factual alignment)</li> <li> <p>Present options:</p> </li> <li> <p>reduce scope</p> </li> <li>increase timeline</li> <li>increase budget</li> <li>reduce complexity (alternative approach)</li> <li>Provide value \u00d7 complexity prioritization</li> <li>Align on decision</li> </ol> <p>This is not a renegotiation. It is a collaborative decision-making event.</p>"},{"location":"204-faq/#6-what-this-looks-like-in-practice-example","title":"6. What This Looks Like in Practice (Example)","text":""},{"location":"204-faq/#without-the-model","title":"Without the model:","text":"<ul> <li>Team feels pressure</li> <li>Timeline slipping</li> <li>Client angry</li> <li>PM defensive</li> <li>\u201cWe underestimated \u2014 sorry\u201d</li> <li>Escalation triggered</li> </ul>"},{"location":"204-faq/#with-the-model","title":"With the model:","text":"<ul> <li>PM: \u201cIntegration complexity increased by +4 due to undocumented API behavior. We detected it early.\u201d</li> <li>PL: \u201cWe already adjusted slice sizes and ran a spike.\u201d</li> <li>Architect: \u201cWe propose an alternative approach reducing complexity by 2.\u201d</li> <li>PM: \u201cThis means a +1.5 week impact unless we de-scope Feature B.\u201d</li> <li>Client: \u201cClear. Let\u2019s de-scope Feature B for now.\u201d</li> </ul> <p>This is how trust is built.</p>"},{"location":"204-faq/#7-when-not-to-escalate","title":"7. When NOT To Escalate","text":"<p>You do not escalate when:</p> <ul> <li>uncertainty resolves without large impact</li> <li>drift is &lt;10%</li> <li>delivery can absorb it within normal sprint variability</li> <li>design improvements add small complexity</li> <li>implementation nuance increases SP but not complexity</li> <li>the team rebalances slices internally</li> </ul> <p>These are normal delivery dynamics.</p>"},{"location":"204-faq/#8-when-you-must-escalate-structured-not-emotional","title":"8. When You MUST Escalate (Structured, Not Emotional)","text":"<p>Escalate when:</p> <ul> <li>drift \u2265 25\u201330%</li> <li>dependencies fail repeatedly</li> <li>major architectural unknowns emerge</li> <li>client-side processes delay acceptance</li> <li>new scope is large or critical</li> <li>product direction shifts</li> <li>UX redesign adds &gt;20\u201330% complexity</li> </ul> <p>But escalate with structure, not emotion:</p> <ul> <li>baseline \u2192 drift \u2192 drivers \u2192 options \u2192 decisions</li> </ul> <p>This is escalation as a professional tool, not a conflict.</p>"},{"location":"204-faq/#9-how-3sf-keeps-drift-conversations-healthy","title":"9. How 3SF Keeps Drift Conversations Healthy","text":""},{"location":"204-faq/#engagement-line-client-vendor","title":"Engagement Line (client \u2194 vendor)","text":"<p>Transparent assumptions avoid blame.</p>"},{"location":"204-faq/#delivery-line-vendor-product","title":"Delivery Line (vendor \u2194 product)","text":"<p>Drift signals appear early, not at crisis time.</p>"},{"location":"204-faq/#value-line-product-client","title":"Value Line (product \u2194 client)","text":"<p>Trade-offs are tied to value, not pain.</p> <p>3SF + Complexity Model = predictable, adult conversations.</p>"},{"location":"205-faq/","title":"How Does This Model Scale in Multi-Team or Multi-Vendor Environments?","text":"<p>The model scales by giving all teams and vendors a shared set of complexity &amp; uncertainty drivers, a unified baseline, a shared dependency map, and a program-level drift dashboard. Each team estimates independently, but the system aligns at the program level \u2014 creating predictable multi-team delivery.</p> <p>How to align complexity, uncertainty, forecasting, and delivery when multiple teams or vendors build a single product.</p>"},{"location":"205-faq/#1-why-this-question-matters","title":"1. Why This Question Matters","text":"<p>Scaling projects across multiple teams or vendors creates systemic complexity:</p> <ul> <li>teams estimate differently</li> <li>vendors use different methods</li> <li>integration points multiply</li> <li>dependencies create delays</li> <li>architectural ownership becomes fragmented</li> <li>discovery happens unevenly</li> <li>velocity becomes incomparable</li> <li>drift propagates across teams</li> <li>clients struggle to understand where the real risks lie</li> </ul> <p>Without a unifying model, multi-team projects become coordination chaos.</p> <p>The Complexity &amp; Uncertainty Framework creates a common language across all participants \u2014 independent of tooling, geography, or methodology.</p>"},{"location":"205-faq/#2-the-core-principle-local-autonomy-shared-model","title":"2. The Core Principle: Local Autonomy, Shared Model","text":"<p>Each team or vendor can still use:</p> <ul> <li>their own story point scale</li> <li>their own refinement rituals</li> <li>their own internal estimation approach</li> <li>their own tooling (Jira, Azure DevOps, Linear, etc.)</li> </ul> <p>But all teams must connect to:</p> <ul> <li>a shared set of complexity drivers</li> <li>a shared uncertainty scoring system</li> <li>a shared complexity baseline</li> <li>shared drift tracking</li> <li>shared integration dependency structure</li> <li>shared trade-off rules</li> </ul> <p>This preserves autonomy while enabling predictability.</p>"},{"location":"205-faq/#3-what-scaling-adds-and-why-its-hard","title":"3. What Scaling Adds (and Why It\u2019s Hard)","text":"<p>Large-scale delivery amplifies challenges:</p>"},{"location":"205-faq/#a-each-team-sees-only-a-piece-of-the-complexity","title":"A. Each team sees only a piece of the complexity","text":"<p>Backend sees backend complexity. Frontend sees frontend complexity. Vendor A sees their part; Vendor B sees theirs. Nobody sees the whole system.</p>"},{"location":"205-faq/#b-dependencies-change-everything","title":"B. Dependencies change everything","text":"<p>A two-team dependency can turn a small feature into a large one.</p>"},{"location":"205-faq/#c-misaligned-assumptions-propagate","title":"C. Misaligned assumptions propagate","text":"<p>If one team assumes \u201csimple API,\u201d another assumes \u201ccomplex API,\u201d both timelines collapse.</p>"},{"location":"205-faq/#d-different-vendors-use-different-estimation-cultures","title":"D. Different vendors use different estimation cultures","text":"<p>This is the biggest source of friction in multi-vendor environments.</p>"},{"location":"205-faq/#e-drift-compounds","title":"E. Drift compounds","text":"<p>If each team\u2019s drift is small but uncoordinated \u2192 total drift becomes massive.</p> <p>The model solves these issues by making complexity and uncertainty visible and shared.</p>"},{"location":"205-faq/#4-how-the-model-scales-step-by-step","title":"4. How the Model Scales (Step-by-Step)","text":"<p>Below is the exact operational flow for multi-team or multi-vendor alignment.</p>"},{"location":"205-faq/#step-1-shared-complexity-drivers-across-all-teams","title":"\u2b50 Step 1 \u2014 Shared Complexity Drivers Across All Teams","text":"<p>Regardless of vendor or location, all teams use the same:</p> <ul> <li>8\u201310 complexity drivers</li> <li>0\u20133\u20135 scoring scale</li> <li>uncertainty scoring model</li> </ul> <p>This creates comparable units of complexity across teams.</p> <p>Vendors keep their internal methods \u2014 but complexity becomes the universal interface.</p>"},{"location":"205-faq/#step-2-unified-complexity-baseline-at-program-level","title":"\u2b50 Step 2 \u2014 Unified Complexity Baseline at Program Level","text":"<p>The program (or client) owns:</p> <ul> <li>Complexity Baseline v1 (RFP)</li> <li>Complexity Baseline v2 (post-discovery)</li> <li>Complexity Baseline v3+ (during delivery)</li> </ul> <p>Each EPIC or capability has:</p> <ul> <li>feature owner</li> <li>contributing teams</li> <li>integration dependencies</li> <li>complexity score per team</li> <li>uncertainty score per team</li> </ul> <p>This gives leadership visibility into the whole system.</p>"},{"location":"205-faq/#step-3-team-level-scoring-feeds-the-program-baseline","title":"\u2b50 Step 3 \u2014 Team-Level Scoring Feeds the Program Baseline","text":"<p>Each team scores their slice:</p> <p>Example EPIC: \u201cEnterprise ERP Integration\u201d</p> Team Complexity Uncertainty Key Drivers Vendor A (Backend) 8 3 Integration, Data Vendor B (Frontend) 3 2 UI, UX Client Platform Team 5 5 NFR, Ownership Data Team 7 4 Data Quality <p>The program sees the real cost of integration \u2014 not assumptions from a single team.</p>"},{"location":"205-faq/#step-4-shared-dependency-map","title":"\u2b50 Step 4 \u2014 Shared Dependency Map","text":"<p>Dependencies get a structured format:</p> <ul> <li>API provider \u2194 API consumer</li> <li>Event producer \u2194 subscriber</li> <li>UX flow \u2194 backend capabilities</li> <li>Data pipeline \u2194 data consumer</li> <li>External vendor \u2194 internal vendor</li> <li>Environment provider \u2194 dev teams</li> </ul> <p>Dependency scoring includes:</p> <ul> <li>ownership clarity</li> <li>SLA predictability</li> <li>readiness</li> <li>documentation quality</li> </ul> <p>This replaces \u201cdependency guessing.\u201d</p>"},{"location":"205-faq/#step-5-drift-tracking-happens-at-both-levels","title":"\u2b50 Step 5 \u2014 Drift Tracking Happens at Both Levels","text":""},{"location":"205-faq/#team-level-drift","title":"Team-Level Drift:","text":"<ul> <li>\u201cBackend integration complexity +3\u201d</li> <li>\u201cFrontend flow simplified \u22121\u201d</li> <li>\u201cData validation expanded +2\u201d</li> </ul>"},{"location":"205-faq/#program-level-drift","title":"Program-Level Drift:","text":"<ul> <li>aggregate drift</li> <li>cross-team contradictions</li> <li>dependency-driven changes</li> <li>impact on release plan</li> </ul> <p>Drift stops being invisible.</p>"},{"location":"205-faq/#step-6-program-level-forecasting-becomes-realistic","title":"\u2b50 Step 6 \u2014 Program-Level Forecasting Becomes Realistic","text":"<p>Instead of averaging velocities across vendors (which is meaningless), forecasting uses:</p> <ul> <li>complexity of upcoming features</li> <li>complexity already delivered</li> <li>uncertainty remaining</li> <li>cycle-time distributions</li> <li>dependency delays</li> </ul> <p>Multi-team forecasting becomes evidence-based.</p>"},{"location":"205-faq/#step-7-trade-offs-become-informed-fair","title":"\u2b50 Step 7 \u2014 Trade-Offs Become Informed &amp; Fair","text":"<p>When new scope appears or complexity grows:</p> <ul> <li>impact is quantified</li> <li>contribution per team is visible</li> <li>negotiation is based on facts</li> <li>vendors are not blamed</li> <li>compromises are not political</li> </ul> <p>This dramatically reduces cross-vendor tension.</p>"},{"location":"205-faq/#5-how-roles-align-in-multi-team-delivery","title":"5. How Roles Align in Multi-Team Delivery","text":""},{"location":"205-faq/#program-manager-pmo","title":"Program Manager / PMO","text":"<p>Owns:</p> <ul> <li>complexity baseline</li> <li>drift dashboard</li> <li>dependency structure</li> <li>forecasting</li> <li>integrated release plan</li> </ul>"},{"location":"205-faq/#architect-across-vendors","title":"Architect (across vendors)","text":"<p>Owns:</p> <ul> <li>system complexity</li> <li>integration scoring</li> <li>NFR alignment</li> <li>dependency resolution</li> </ul>"},{"location":"205-faq/#product-manager","title":"Product Manager","text":"<p>Owns:</p> <ul> <li>value vs complexity prioritization</li> <li>feature sequencing</li> <li>stakeholder alignment</li> </ul>"},{"location":"205-faq/#engineering-leads-per-team","title":"Engineering Leads (per team)","text":"<p>Own:</p> <ul> <li>local complexity scores</li> <li>local drift tracking</li> <li>SP mapping</li> <li>readiness buffer</li> </ul>"},{"location":"205-faq/#vendor-leads","title":"Vendor Leads","text":"<p>Own:</p> <ul> <li>cohesion within the vendor team</li> <li>honest alignment with client baseline</li> <li>transparent reporting</li> </ul>"},{"location":"205-faq/#6-how-this-model-supports-multi-vendor-trust-3sf-alignment","title":"6. How This Model Supports Multi-Vendor Trust (3SF Alignment)","text":""},{"location":"205-faq/#engagement-line-client-vendors","title":"Engagement Line (client \u2194 vendor(s))","text":"<p>Complexity drivers make assumptions visible across all parties. No vendor is \u201cguessing wrong.\u201d</p>"},{"location":"205-faq/#delivery-line-vendor-product","title":"Delivery Line (vendor \u2194 product)","text":"<p>Drift detection reduces conflict between vendors. Dependencies are transparent.</p>"},{"location":"205-faq/#value-line-product-client","title":"Value Line (product \u2194 client)","text":"<p>Trade-offs are clear and fair. Value delivery becomes predictable.</p> <p>3SF becomes the connective tissue between vendors.</p>"},{"location":"205-faq/#7-when-multi-vendor-alignment-fails-and-how-this-model-prevents-it","title":"7. When Multi-Vendor Alignment Fails (and How This Model Prevents It)","text":""},{"location":"205-faq/#failure-modes","title":"Failure Modes:","text":"<ul> <li>contradictory estimates</li> <li>finger-pointing</li> <li>unclear dependencies</li> <li>mismatched assumptions</li> <li>unrealistic shared timelines</li> <li>RFP underestimation due to missing team input</li> </ul>"},{"location":"205-faq/#model-prevents-by","title":"Model Prevents by:","text":"<ul> <li>shared driver scoring</li> <li>shared dependency maps</li> <li>shared baselines</li> <li>shared drift dashboards</li> <li>structured trade-offs</li> </ul> <p>It turns chaos into a coordinated system.</p>"},{"location":"206-faq/","title":"How Estimation Aligns Across Dual-Track Scrum (Discovery + Delivery)?","text":"<p>Discovery estimates complexity &amp; uncertainty, Delivery estimates slices &amp; story points, and PM/PL keep both tracks aligned by maintaining a shared Complexity Baseline, a readiness buffer, and weekly track syncs. This makes Dual-Track predictable, measurable, and aligned end-to-end.</p> <p>How PM and PL keep both tracks synchronized using Complexity &amp; Uncertainty.</p> <p>Understood \u2014 here is a compact version matching the length and style of the other FAQ intros:</p>"},{"location":"206-faq/#1-why-this-question-matters","title":"1. Why This Question Matters","text":"<p>Dual-Track Scrum means:</p> <ul> <li>Discovery track continuously clarifies what to build</li> <li>Delivery track continuously builds what is ready</li> </ul> <p>The two tracks operate in parallel, but they do not estimate the same way and they absolutely need a shared coordination model.</p> <p>Dual-Track Scrum often fails not because of the idea, but because estimation, readiness, and uncertainty are tracked differently in each stream. Discovery works with ambiguity and problem exploration; Delivery works with sliced work and predictable flow. Without a shared model, the tracks drift:</p> <ul> <li>Discovery outputs aren\u2019t ready when Delivery needs them</li> <li>Delivery discovers hidden complexity too late</li> <li>Uncertainty isn\u2019t visible across tracks</li> <li>Planning becomes reactive instead of intentional</li> </ul> <p>This question matters because Dual-Track only succeeds when both tracks speak the same estimation language and connect their work through shared complexity, uncertainty, and baseline signals.</p> <p>The Complexity &amp; Uncertainty framework acts as that shared model.</p> <p>Here\u2019s how it works.</p>"},{"location":"206-faq/#2-dual-track-roles-in-estimation","title":"2. Dual-Track Roles in Estimation","text":""},{"location":"206-faq/#discovery-track-upstream","title":"Discovery Track (Upstream)","text":"<p>Owned by: PM, BA, PD, Architect Focus: Understanding the problem, clarifying requirements, reducing uncertainty.</p>"},{"location":"206-faq/#delivery-track-downstream","title":"Delivery Track (Downstream)","text":"<p>Owned by: PL (Engineering Manager or Tech Lead), Delivery Team Focus: Implementing validated features, estimating slices/stories, controlling flow.</p>"},{"location":"206-faq/#3-what-each-track-estimates","title":"3. What Each Track Estimates","text":""},{"location":"206-faq/#discovery-estimates","title":"Discovery estimates:","text":"<ul> <li>Complexity (drivers) \u2192 integration, data, UX, logic, dependencies</li> <li>Uncertainty \u2192 requirements, documentation gaps, PoCs needed</li> <li>Discovery workload \u2192 PM/BA/PD/Architect time</li> <li>Baseline adjustments \u2192 after clarifying reality</li> </ul> <p>This track feeds Delivery.</p>"},{"location":"206-faq/#delivery-estimates","title":"Delivery estimates:","text":"<ul> <li>Story point ranges (derived from complexity baseline)</li> <li>Cycle times</li> <li>Throughput / WIP</li> <li>Sprint commitments</li> <li>Flow risks</li> </ul> <p>This track executes against Discovery outputs.</p>"},{"location":"206-faq/#4-how-pm-pl-align-both-tracks-practical-workflow","title":"4. How PM &amp; PL Align Both Tracks (Practical Workflow)","text":"<p>Below is the alignment loop that keeps Discovery and Delivery synchronized and predictable.</p>"},{"location":"206-faq/#step-1-discovery-produces-the-complexity-uncertainty-scores","title":"\u2b50 Step 1 \u2014 Discovery produces the Complexity &amp; Uncertainty scores","text":"<p>This happens before a feature enters Delivery.</p> <p>Discovery generates:</p> <ul> <li>Complexity Score</li> <li>Uncertainty Score</li> <li>Updated Complexity Baseline (v2/v3)</li> <li>Clarified requirements &amp; UX flows</li> <li>Spike outcomes (if needed)</li> </ul> <p>This ensures Delivery never estimates blindly.</p>"},{"location":"206-faq/#step-2-pm-pl-meet-weekly-for-a-track-sync","title":"\u2b50 Step 2 \u2014 PM &amp; PL meet weekly for a \u201cTrack Sync\u201d","text":"<p>Agenda:</p> <ol> <li>What\u2019s ready to enter Delivery?</li> <li>Has any complexity drift occurred?</li> <li>Is uncertainty decreasing as expected?</li> <li>Are dependencies becoming clearer or riskier?</li> <li>Is Discovery producing enough ready work for upcoming sprints?</li> </ol> <p>This replaces the old \u201crefinement chaos\u201d with a structured alignment loop.</p>"},{"location":"206-faq/#step-3-delivery-maps-incoming-items-to-story-points","title":"\u2b50 Step 3 \u2014 Delivery maps incoming items to Story Points","text":"<p>Story points are not estimated independently. They\u2019re anchored to the complexity baseline:</p> Complexity SP Range Low 1\u20133 Medium 5\u20138 High 13\u201321 Extreme Split <p>This keeps Delivery honest and prevents point inflation.</p>"},{"location":"206-faq/#step-4-pm-tracks-discovery-burn-up-vs-delivery-burn-down","title":"\u2b50 Step 4 \u2014 PM tracks Discovery Burn-up vs. Delivery Burn-down","text":"<p>Discovery should run ahead of Delivery by at least 1\u20132 sprints.</p> <p>PM tracks:</p> <ul> <li>number of clarified features</li> <li>uncertainty reduction</li> <li>upcoming complexity</li> <li>readiness for slicing</li> </ul> <p>PL tracks:</p> <ul> <li>sprint commitments</li> <li>throughput</li> <li>cycle time</li> <li>capacity risks</li> </ul> <p>The PM and PL jointly maintain a shared \u201cReadiness Buffer.\u201d</p> <p>This is key in Dual-Track environments.</p>"},{"location":"206-faq/#step-5-both-tracks-update-the-complexity-baseline-as-new-information-emerges","title":"\u2b50 Step 5 \u2014 Both tracks update the Complexity Baseline as new information emerges","text":"<p>Discovery updates it when clarity increases. Delivery updates it when drift is discovered.</p> <p>The baseline sits at the midpoint between tracks \u2014 the shared \u201ctruth.\u201d</p>"},{"location":"206-faq/#step-6-pm-pl-realign-timelines-based-on-complexity-uncertainty-changes","title":"\u2b50 Step 6 \u2014 PM &amp; PL realign timelines based on complexity &amp; uncertainty changes","text":"<p>Not velocity.</p> <p>Velocity is a capacity signal, not a planning tool.</p> <p>Instead, forecasting uses:</p> <ul> <li>updated complexity totals</li> <li>updated uncertainty levels</li> <li>updated batch sizes</li> <li>delivery throughput</li> </ul> <p>This allows for transparent communication with stakeholders.</p>"},{"location":"206-faq/#5-flow-of-artifacts-across-tracks","title":"5. Flow of Artifacts Across Tracks","text":""},{"location":"206-faq/#from-discovery-delivery","title":"From Discovery \u2192 Delivery","text":"<ul> <li>feature descriptions</li> <li>UX flows</li> <li>acceptance criteria</li> <li>complexity scores</li> <li>uncertainty scores</li> <li>dependencies map</li> <li>ready-for-delivery signals (DoR)</li> </ul>"},{"location":"206-faq/#from-delivery-discovery","title":"From Delivery \u2192 Discovery","text":"<ul> <li>complexity drift signals</li> <li>validation feedback</li> <li>cycle-time insights</li> <li>integration blockers</li> <li>refinement requests</li> </ul> <p>This bi-directional flow is the connective tissue of Dual-Track.</p>"},{"location":"206-faq/#6-where-pm-and-pl-responsibilities-overlap-vs-divide","title":"6. Where PM and PL Responsibilities Overlap vs. Divide","text":"Activity PM (Discovery Owner) PL (Delivery Owner) Shared Define complexity drivers \u2714\ufe0f \u2796 \u2714\ufe0f Score uncertainty \u2714\ufe0f \u2796 \u2714\ufe0f Create Complexity Baseline \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f Maintain readiness buffer \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f Create Story Points \u2796 \u2714\ufe0f \u2714\ufe0f Track complexity drift \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f Forecast timelines \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f Manage sprint flow \u2796 \u2714\ufe0f \u2796 Manage Discovery cadence \u2714\ufe0f \u2796 \u2796 <p>This table clarifies how Dual-Track roles share ownership.</p>"},{"location":"206-faq/#7-why-this-model-fits-3sf-perfectly","title":"7. Why This Model Fits 3SF Perfectly","text":""},{"location":"206-faq/#engagement-client-vendor","title":"Engagement (Client \u2194 Vendor)","text":"<p>Discovery provides realistic expectations through complexity &amp; uncertainty visibility.</p>"},{"location":"206-faq/#delivery-vendor-product","title":"Delivery (Vendor \u2194 Product)","text":"<p>Dual-track synchronization reduces surprises and removes pressure to \u201cincrease velocity.\u201d</p>"},{"location":"206-faq/#value-product-client","title":"Value (Product \u2194 Client)","text":"<p>Clear trade-offs emerge because complexity is quantified early.</p> <p>The dual-track sync is essentially a 3SF \u201cflow stabilizer.\u201d</p>"},{"location":"207-faq/","title":"Does Tracking Complexity and Uncertainty Increase Overhead? How Do We Keep It Lightweight?","text":"<p>No \u2014 tracking complexity and uncertainty does not add overhead. It replaces invisible, high-cost overhead (rework, re-estimation, surprises) with a fast, lightweight alignment tool. Teams score only meaningful features, only when needed, using a simple scale \u2014 and save significant effort later in delivery.</p> <p>How to make the model efficient, fast, and practical \u2014 not bureaucratic.</p>"},{"location":"207-faq/#1-why-this-question-exists","title":"1. Why This Question Exists","text":"<p>Whenever a new practice is introduced, especially one involving scoring or alignment, teams naturally worry about:</p> <ul> <li>\u201cIs this more paperwork?\u201d</li> <li>\u201cWill this slow us down?\u201d</li> <li>\u201cDo we really need another meeting?\u201d</li> <li>\u201cIs this replacing agility with process?\u201d</li> <li>\u201cWill this add friction for PM/BA/PD/Architects?\u201d</li> </ul> <p>These concerns are legitimate \u2014 and they mostly come from prior experiences where estimation approaches became heavy and ritualistic.</p> <p>This model is intentionally built to be lightweight, fast, and minimal overhead, while providing deep visibility.</p> <p>This page explains how.</p>"},{"location":"207-faq/#2-the-core-principle-minimum-effort-maximum-signal","title":"2. The Core Principle: Minimum Effort, Maximum Signal","text":"<p>The model is not designed to be:</p> <ul> <li>a big spreadsheet</li> <li>an admin-heavy ritual</li> <li>a governance checkpoint</li> <li>a status-reporting mechanism</li> <li>a re-estimation ceremony</li> </ul> <p>It is designed as a quick sensing mechanism \u2014 a radar, not bureaucracy.</p> <p>You score only what matters, at the level where decisions are made.</p>"},{"location":"207-faq/#3-where-teams-fear-overhead-and-why-it-doesnt-happen","title":"3. Where Teams Fear Overhead \u2014 And Why It Doesn\u2019t Happen","text":""},{"location":"207-faq/#fear-1-we-will-score-complexity-for-every-story","title":"Fear 1 \u2014 \u201cWe will score complexity for every story.\u201d","text":"<p>No. You score only EPICs, features, or slices \u2014 not tasks.</p>"},{"location":"207-faq/#fear-2-we-will-re-score-all-the-time","title":"Fear 2 \u2014 \u201cWe will re-score all the time.\u201d","text":"<p>No. Baseline updates happen only when drift is significant (e.g., &gt;15\u201320%). Most work requires no re-scoring.</p>"},{"location":"207-faq/#fear-3-a-scoring-session-will-take-hours","title":"Fear 3 \u2014 \u201cA scoring session will take hours.\u201d","text":"<p>No. Scoring is designed to take:</p> <ul> <li>5\u201310 minutes per feature for RFP</li> <li>1\u20132 minutes per feature during discovery</li> <li>30 seconds during drift review</li> </ul>"},{"location":"207-faq/#fear-4-pms-or-bas-will-own-the-entire-burden","title":"Fear 4 \u2014 \u201cPMs or BAs will own the entire burden.\u201d","text":"<p>No. Scoring is distributed across roles \u2014 each person scores what they know best.</p>"},{"location":"207-faq/#fear-5-this-will-overlap-existing-meetings","title":"Fear 5 \u2014 \u201cThis will overlap existing meetings.\u201d","text":"<p>No. It integrates seamlessly with:</p> <ul> <li>discovery</li> <li>refinement</li> <li>PM/PL sync</li> <li>architecture sync</li> <li>sprint review</li> </ul> <p>No new ceremonies needed.</p>"},{"location":"207-faq/#4-how-the-model-stays-lightweight-mechanics","title":"4. How the Model Stays Lightweight (Mechanics)","text":""},{"location":"207-faq/#1-complexity-drivers-are-intentionally-simple","title":"\u2b50 1. Complexity drivers are intentionally simple","text":"<p>9\u201310 drivers. 0\u20133\u20135 scale. No formulas. No ambiguity.</p>"},{"location":"207-faq/#2-scoring-is-done-collaboratively-not-sequentially","title":"\u2b50 2. Scoring is done collaboratively, not sequentially","text":"<p>Each person scores their dimension simultaneously. Takes minutes, not hours.</p>"},{"location":"207-faq/#3-only-score-at-the-unit-of-meaning-level","title":"\u2b50 3. Only score at the \u201cunit of meaning\u201d level","text":"<p>Examples:</p> <ul> <li>EPICs</li> <li>capabilities</li> <li>features</li> <li>integration blocks</li> <li>UX flows</li> <li>slices   Not:</li> <li>tasks</li> <li>subtasks</li> <li>story-level items</li> </ul>"},{"location":"207-faq/#4-drift-is-flagged-by-pmpl-not-scored-constantly","title":"\u2b50 4. Drift is flagged by PM/PL, not scored constantly","text":"<p>If nothing changes \u2192 no work. If something changes \u2192 re-score only the affected driver.</p>"},{"location":"207-faq/#5-baseline-updates-are-infrequent","title":"\u2b50 5. Baseline updates are infrequent","text":"<p>v1 \u2192 RFP v2 \u2192 after discovery v3+ \u2192 only when drift is significant Most projects have 2\u20133 baseline versions total, not dozens.</p>"},{"location":"207-faq/#6-uncertainty-scoring-reduces-rework","title":"\u2b50 6. Uncertainty scoring reduces rework","text":"<p>When teams know what\u2019s unclear, they stop:</p> <ul> <li>overbuilding</li> <li>overestimating</li> <li>underestimating</li> <li>gambling</li> <li>guessing</li> </ul> <p>This reduces overhead instead of adding it.</p>"},{"location":"207-faq/#5-overhead-comparison-before-vs-after","title":"5. Overhead Comparison: Before vs After","text":""},{"location":"207-faq/#before-the-model-hidden-overhead","title":"Before the model (hidden overhead):","text":"<ul> <li>endless refinement</li> <li>unclear scope</li> <li>re-estimation loops</li> <li>velocity manipulation</li> <li>misaligned dependencies</li> <li>sprint-to-sprint firefighting</li> <li>late discovery</li> <li>last-minute scope cuts</li> <li>PM/BA/Architect burnout</li> </ul> <p>This is enormous overhead \u2014 but invisible.</p>"},{"location":"207-faq/#after-the-model-visible-minimal-overhead","title":"After the model (visible, minimal overhead):","text":"<ul> <li>quick feature scoring</li> <li>early uncertainty detection</li> <li>predictable drift identification</li> <li>clear alignment</li> <li>fewer surprises</li> <li>less rework</li> <li>smoother sprint planning</li> <li>reduced conflict</li> <li>fewer escalations</li> </ul> <p>This is low overhead \u2014 and high leverage.</p>"},{"location":"207-faq/#6-where-the-efficiency-comes-from","title":"6. Where the Efficiency Comes From","text":"<p>Tracking complexity &amp; uncertainty:</p> <ul> <li>reduces rework (major savings)</li> <li>prevents late-stage surprises (huge savings)</li> <li>reduces PM/BA/Dev/Architect firefighting (massive savings)</li> <li>prevents 5+ rounds of story re-estimation (priceless savings)</li> <li>minimizes context switching</li> <li>reduces pressure to \u201cgo faster\u201d</li> <li>stabilizes velocity</li> <li>removes bureaucratic process that grew to compensate for lack of clarity</li> </ul> <p>It\u2019s one of the rare practices where you invest minutes and save weeks.</p>"},{"location":"207-faq/#7-practical-ways-to-keep-it-light","title":"7. Practical Ways to Keep It Light","text":"<p>Here is the operational guideline:</p>"},{"location":"207-faq/#only-score-when-something-changes","title":"\u2714 Only score when something changes","text":"<p>If nothing changed \u2192 no action.</p>"},{"location":"207-faq/#score-only-what-matters","title":"\u2714 Score only what matters","text":"<p>EPICs/features \u2014 not stories/tasks.</p>"},{"location":"207-faq/#score-fast","title":"\u2714 Score fast","text":"<p>Use 0\u20133\u20135 scale to avoid debate.</p>"},{"location":"207-faq/#do-it-in-regular-meetings","title":"\u2714 Do it in regular meetings","text":"<p>Discovery sync \u2192 RFP &amp; clarification Refinement \u2192 slicing PM/PL sync \u2192 drift review Sprint Review \u2192 reflection</p>"},{"location":"207-faq/#make-it-a-shared-responsibility","title":"\u2714 Make it a shared responsibility","text":"<p>Not PM alone. Not Architect alone. Everyone contributes small pieces.</p>"},{"location":"207-faq/#use-visual-dashboards","title":"\u2714 Use visual dashboards","text":"<p>Complexity drift graphs in Confluence/Jira reduce meetings.</p>"},{"location":"207-faq/#dont-chase-precision","title":"\u2714 Don\u2019t chase precision","text":"<p>We\u2019re seeking signals, not \u201ccorrectness.\u201d</p>"},{"location":"208-faq/","title":"How Does This Model Help Us Make Scope, Sequencing, and Value-Tradeoff Decisions With Clients?","text":"<p>The model enables clear, evidence-based decisions about scope, sequencing, and value by making complexity, uncertainty, and drift visible. It turns trade-off discussions into structured, data-driven conversations rather than subjective debates \u2014 and helps clients make informed decisions.</p> <p>How complexity &amp; uncertainty scoring transforms negotiations from emotional to evidence-based conversations.</p>"},{"location":"208-faq/#1-why-this-question-matters","title":"1. Why This Question Matters","text":"<p>It is difficult to have productive conversations with clients about:</p> <ul> <li>scope cuts</li> <li>timeline adjustments</li> <li>feature sequencing</li> <li>MVP definition</li> <li>what to build now vs later</li> <li>what is \u201cworth it\u201d</li> <li>why complexity increased</li> <li>why a feature should move or be dropped</li> </ul> <p>Without a structured model, these conversations often become:</p> <ul> <li>intuitive</li> <li>emotional</li> <li>political</li> <li>subjective</li> <li>conflict-prone</li> </ul> <p>Because the team can\u2019t show the why behind their recommendations.</p> <p>The Complexity &amp; Uncertainty Framework turns trade-offs into facts, not opinions.</p>"},{"location":"208-faq/#2-the-core-principle-we-shift-from-how-we-feel-what-the-data-shows","title":"2. The Core Principle: We Shift From \u201cHow We Feel\u201d \u2192 \u201cWhat the Data Shows\u201d","text":"<p>Before the model:</p> <ul> <li>\u201cThis is too complex.\u201d</li> <li>\u201cThis will take longer.\u201d</li> <li>\u201cWe underestimated.\u201d</li> <li>\u201cWe need to cut scope.\u201d</li> </ul> <p>Statements like these are vague and create tension.</p> <p>After the model:</p> <ul> <li>\u201cIntegration complexity increased by +5 due to new API requirements.\u201d</li> <li>\u201cUX branching expanded from 3 \u2192 9 flows.\u201d</li> <li>\u201cUncertainty started high and only resolved last week; the buffer was consumed.\u201d</li> <li>\u201cThis feature became a high-complexity, low-value item \u2014 a strong candidate for de-scoping.\u201d</li> </ul> <p>Now decisions are grounded in structured input.</p>"},{"location":"208-faq/#3-how-the-model-enables-data-driven-trade-offs","title":"3. How the Model Enables Data-Driven Trade-Offs","text":"<p>Below are eight mechanisms that directly support scope and sequencing decisions.</p>"},{"location":"208-faq/#mechanism-1-value-complexity-matrix","title":"\u2b50 Mechanism 1 \u2014 Value \u00d7 Complexity Matrix","text":"<p>Every feature gets:</p> <ul> <li>a value level</li> <li>a complexity score</li> </ul> <p>This makes trade-offs visible:</p> Value Low Complexity Medium High Extreme High Build first Build first Discuss alternatives Redesign or delay Medium Build soon Consider sequencing Delay Push out Low Maybe include Defer De-scope Remove <p>This matrix becomes the map of sequencing strategy.</p>"},{"location":"208-faq/#mechanism-2-complexity-drift-becomes-a-signal-not-a-crisis","title":"\u2b50 Mechanism 2 \u2014 Complexity Drift Becomes a Signal, Not a Crisis","text":"<p>With drift tracking:</p> <ul> <li>we see early when a feature is becoming disproportionately expensive</li> <li>we see where the risk accumulates</li> <li>we see where assumptions break</li> <li>we see the trend, not the surprise</li> </ul> <p>Example: \u201cFeature X drifted +18% over two sprints \u2014 strong case to move it out of MVP.\u201d</p> <p>Clients respond to trend data better than emotions.</p>"},{"location":"208-faq/#mechanism-3-uncertainty-burn-down-shows-whats-safe-to-commit","title":"\u2b50 Mechanism 3 \u2014 Uncertainty Burn-Down Shows What\u2019s Safe to Commit","text":"<p>Uncertainty decreasing \u2192 okay to sequence earlier Uncertainty increasing \u2192 postpone, spike, or descoped</p> <p>Example: \u201cFeature Y still has a 9/15 uncertainty score \u2014 building it now is gambling.\u201d</p> <p>Clients understand risk mitigation intuitively.</p>"},{"location":"208-faq/#mechanism-4-explicit-dependency-mapping","title":"\u2b50 Mechanism 4 \u2014 Explicit Dependency Mapping","text":"<p>Clients often underestimate dependency cost.</p> <p>The model makes it visible:</p> <ul> <li>\u201cThis EPIC depends on API Z.\u201d</li> <li>\u201cAPI Z readiness is 3/5 uncertain.\u201d</li> <li>\u201cSequencing Feature A before Feature B will cause 5\u20137 extra dev weeks.\u201d</li> </ul> <p>This changes discussions from:</p> <p>\u201cWhy can\u2019t we build X first?\u201d \u2192 \u201cBuilding X first causes a measurable complexity increase.\u201d</p>"},{"location":"208-faq/#mechanism-5-feature-clustering-for-strategic-releases","title":"\u2b50 Mechanism 5 \u2014 Feature Clustering for Strategic Releases","text":"<p>Complexity scoring naturally groups features into clusters:</p> <ul> <li>low-complexity enabling features</li> <li>medium-complexity value-carriers</li> <li>high-complexity architecture-heavy blocks</li> </ul> <p>This reveals natural release candidates.</p> <p>Clients see that:</p> <ul> <li>building all high-complexity features at once is dangerous</li> <li>mixing complexity levels makes releases stable</li> <li>complexity sequencing reduces risk</li> </ul>"},{"location":"208-faq/#mechanism-6-transparency-strengthens-trust","title":"\u2b50 Mechanism 6 \u2014 Transparency Strengthens Trust","text":"<p>Clients react well when they see:</p> <ul> <li>assumptions</li> <li>drivers</li> <li>clarity</li> <li>signals</li> <li>reasoned recommendations</li> </ul> <p>The PM and PL stop sounding subjective, and instead sound like:</p> <ul> <li>advisors</li> <li>strategists</li> <li>partners</li> </ul> <p>This elevates the relationship from tactical \u2192 strategic.</p>"},{"location":"208-faq/#mechanism-7-options-not-ultimatums","title":"\u2b50 Mechanism 7 \u2014 Options, Not Ultimatums","text":"<p>Instead of \u201cWe must delay the release,\u201d the model empowers PM/PL to present options:</p> <p>\u201cBased on complexity drift, you have three options: A) keep scope, extend by 2 weeks B) reduce scope by X C) explore a lower-complexity alternative\u201d  </p> <p>Clients love when they\u2019re given choices, not constraints.</p>"},{"location":"208-faq/#mechanism-8-fairness-and-objectivity-across-vendors","title":"\u2b50 Mechanism 8 \u2014 Fairness and Objectivity Across Vendors","text":"<p>In multi-vendor environments, this model:</p> <ul> <li>prevents finger-pointing</li> <li>normalizes assumptions</li> <li>drives fair conversations</li> <li>makes complexity visible across teams</li> <li>creates shared expectations</li> </ul> <p>This improves cross-vendor alignment and simplifies negotiation.</p>"},{"location":"208-faq/#4-how-pm-pl-use-the-model-in-real-conversations","title":"4. How PM &amp; PL Use the Model in Real Conversations","text":""},{"location":"208-faq/#pm-uses-it-to","title":"PM uses it to:","text":"<ul> <li>communicate uncertainty</li> <li>justify sequencing</li> <li>show value-to-effort trade-offs</li> <li>explain drift</li> <li>protect the roadmap</li> <li>guide client expectations</li> </ul>"},{"location":"208-faq/#pl-uses-it-to","title":"PL uses it to:","text":"<ul> <li>explain technical drivers</li> <li>propose alternative solutions</li> <li>validate complexity changes</li> <li>protect engineering capacity</li> <li>avoid unhealthy velocity pressure</li> </ul> <p>Together, PM + PL become a unified advisory pair.</p>"},{"location":"208-faq/#5-what-this-looks-like-in-practice-example-conversation","title":"5. What This Looks Like in Practice (Example Conversation)","text":"<p>Before the model:</p> <p>\u201cThis feature is harder than expected, so we need to cut something.\u201d</p> <p>Client response: \ud83d\ude20 \u201cWhy didn\u2019t you know earlier?\u201d</p> <p>After the model:</p> <p>PM: \u201cDiscovery reduced uncertainty by 40%, but integration complexity increased by +6 due to new API constraints.\u201d  </p> <p>PL: \u201cGiven the drift, keeping Feature B this release adds +2.5 weeks.\u201d  </p> <p>PM: \u201cHere are your options:  </p> <ol> <li>Extend by 2.5 weeks  </li> <li>Remove Feature B  </li> <li>Replace Feature B with Feature C (similar value, lower complexity)\u201d  </li> </ol> <p>Client: \u201cLet\u2019s go with option 3.\u201d  </p> <p>This is how business alignment happens at a professional level.</p>"},{"location":"209-faq/","title":"What Happens When New Scope Appears Mid-Project \u2014 How Do We Score, Track, and Communicate It?","text":"<p>When new scope appears, we score it using the same complexity and uncertainty drivers, compare it against the baseline, and present clear trade-off options to the client. This keeps delivery stable, protects the team, and turns scope changes into professional, data-driven decisions \u2014 not disruption.</p> <p>A structured way to handle scope additions without chaos, conflict, or timeline collapse.</p>"},{"location":"209-faq/#1-why-this-question-matters","title":"1. Why This Question Matters","text":"<p>Every real project experiences mid-flight scope additions:</p> <ul> <li>new features</li> <li>new workflows</li> <li>new integrations</li> <li>new compliance requirements</li> <li>new UX needs</li> <li>new stakeholder requests</li> <li>\u201ccan we just add X?\u201d</li> <li>\u201cwe forgot Y\u201d</li> <li>\u201cwe discovered Z during delivery\u201d</li> </ul> <p>The challenge is not the new scope itself \u2014 it\u2019s the disruption it causes.</p> <p>Most teams fall into one of two failure modes:</p>"},{"location":"209-faq/#failure-mode-a-absorb-everything","title":"Failure Mode A \u2014 Absorb everything","text":"<p>\u2192 hidden debt \u2192 collapsing timelines \u2192 burnout \u2192 failed expectations \u2192 eroded trust</p>"},{"location":"209-faq/#failure-mode-b-reject-everything","title":"Failure Mode B \u2014 Reject everything","text":"<p>\u2192 stakeholder frustration \u2192 scope battles \u2192 political conflict</p> <p>Neither is professional.</p> <p>This page explains how to handle new scope using the Complexity &amp; Uncertainty Model \u2014 without collapsing the delivery system.</p>"},{"location":"209-faq/#2-core-principle-new-scope-enters-the-system-the-same-way-as-original-scope","title":"2. Core Principle: New Scope Enters the System the Same Way as Original Scope","text":"<p>Instead of treating new scope as a special case:</p> <p>New scope is treated exactly like original scope \u2014 scored, baselined, and evaluated using the same drivers.</p> <p>This creates:</p> <ul> <li>fairness</li> <li>consistency</li> <li>predictability</li> <li>transparency</li> </ul> <p>And eliminates emotional or political responses.</p>"},{"location":"209-faq/#3-when-new-scope-appears-there-are-three-required-steps","title":"3. When New Scope Appears, There Are Three Required Steps","text":""},{"location":"209-faq/#step-1-score-the-new-scope","title":"Step 1 \u2014 Score the new scope","text":""},{"location":"209-faq/#step-2-compare-against-the-baseline","title":"Step 2 \u2014 Compare against the baseline","text":""},{"location":"209-faq/#step-3-present-options-and-trade-offs-to-the-client","title":"Step 3 \u2014 Present options and trade-offs to the client","text":"<p>Everything flows from this.</p> <p>Let\u2019s break these down.</p>"},{"location":"209-faq/#4-step-1-score-the-new-scope-complexity-uncertainty","title":"4. Step 1 \u2014 Score the New Scope (Complexity + Uncertainty)","text":"<p>PM, BA, PD, Architect, and EM/PL quickly score the new item:</p>"},{"location":"209-faq/#complexity-drivers","title":"Complexity drivers:","text":"<ul> <li>integration</li> <li>data</li> <li>UX</li> <li>logic</li> <li>NFR</li> <li>environment</li> <li>external dependencies</li> </ul>"},{"location":"209-faq/#uncertainty-drivers","title":"Uncertainty drivers:","text":"<ul> <li>requirement clarity</li> <li>documentation</li> <li>ownership</li> <li>ambiguity</li> <li>dependency stability</li> </ul> <p>This scoring usually takes 3\u201310 minutes, because:</p> <ul> <li>the model is simple</li> <li>the scoring scale is small (0\u20133\u20135)</li> <li>each role contributes only what they know</li> <li>drivers are familiar across the team</li> </ul> <p>The result is a structured assessment instead of a vague \u201cthis feels big.\u201d</p>"},{"location":"209-faq/#5-step-2-compare-the-new-scope-against-the-baseline","title":"5. Step 2 \u2014 Compare the New Scope Against the Baseline","text":"<p>This reveals its real impact.</p>"},{"location":"209-faq/#evaluate","title":"Evaluate:","text":"<ul> <li>total complexity change</li> <li>uncertainty level</li> <li>integration with existing features</li> <li>cross-team impact</li> <li>drift trend</li> <li>value vs complexity ratio</li> <li>feasibility in the current release</li> </ul>"},{"location":"209-faq/#new-scope-becomes-one-of-four-categories","title":"New scope becomes one of four categories:","text":"Category Meaning Example Action High value, low complexity A good add Fit into release, or swap with something bigger High value, high complexity Needs trade-offs Extend timeline OR drop another feature Low value, low complexity Opportunistic Add only if capacity allows Low value, high complexity Decline Move to backlog or de-scope <p>Instead of \u201cno\u201d or \u201cyes,\u201d the conversation becomes structured decision-making.</p>"},{"location":"209-faq/#6-step-3-present-clear-options-to-the-client","title":"6. Step 3 \u2014 Present Clear Options to the Client","text":"<p>The PM presents the impact factually, with choices, not demands.</p>"},{"location":"209-faq/#example-structure","title":"Example structure:","text":"<ol> <li> <p>What changed</p> </li> <li> <p>\u201cNew compliance workflow added.\u201d</p> </li> <li> <p>\u201cIntegration with vendor X needed.\u201d</p> </li> <li> <p>Complexity &amp; uncertainty score</p> </li> <li> <p>\u201c+8 complexity, +3 uncertainty.\u201d</p> </li> <li> <p>Impact</p> </li> <li> <p>\u201cThis adds 2\u20133 weeks of work.\u201d</p> </li> <li> <p>Options</p> </li> <li> <p>Option A: extend timeline</p> </li> <li>Option B: increase capacity (if possible)</li> <li>Option C: replace Feature B with new Feature X</li> <li> <p>Option D: split X into a smaller version for MVP</p> </li> <li> <p>Recommendation</p> </li> <li> <p>backed by value \u00d7 complexity analysis</p> </li> </ol> <p>Clients trust structured recommendations, not emotional reactions.</p>"},{"location":"209-faq/#7-how-new-scope-fits-into-dual-track-scrum","title":"7. How New Scope Fits Into Dual-Track Scrum","text":""},{"location":"209-faq/#discovery-track","title":"Discovery Track","text":"<p>Quickly clarifies new scope:</p> <ul> <li>requirements</li> <li>UX</li> <li>dependencies</li> <li>uncertainty</li> <li>complexity drivers</li> </ul>"},{"location":"209-faq/#delivery-track","title":"Delivery Track","text":"<p>Receives:</p> <ul> <li>a scored item</li> <li>a ready-for-delivery view</li> <li>a decision (include, swap, delay, decline)</li> </ul> <p>No chaos. No \u201curgent\u201d interruptions. The tracks remain synchronized.</p>"},{"location":"209-faq/#8-how-pm-and-pl-share-the-work","title":"8. How PM and PL Share the Work","text":""},{"location":"209-faq/#pm-responsibilities","title":"PM responsibilities:","text":"<ul> <li>ensure new scope is captured</li> <li>facilitate complexity/uncertainty scoring</li> <li>assess value</li> <li>prepare trade-off options</li> <li>communicate with stakeholders</li> <li>align sequencing</li> <li>protect the roadmap</li> </ul>"},{"location":"209-faq/#pl-em-responsibilities","title":"PL / EM responsibilities:","text":"<ul> <li>validate technical complexity</li> <li>assess delivery impact</li> <li>identify dependency risks</li> <li>propose alternative low-complexity approaches</li> </ul> <p>Together, PM + PL create a shared, unified recommendation.</p>"},{"location":"209-faq/#9-how-this-model-reduces-conflict","title":"9. How This Model Reduces Conflict","text":""},{"location":"209-faq/#without-the-model","title":"Without the model:","text":"<ul> <li>new scope creates panic</li> <li>team absorbs everything</li> <li>stakeholders escalate</li> <li>timeline slips without explanation</li> <li>trust erodes</li> </ul>"},{"location":"209-faq/#with-the-model","title":"With the model:","text":"<ul> <li>new scope is scored</li> <li>impact is quantified</li> <li>trade-offs are clear</li> <li>client chooses path forward</li> <li>timeline adjustments are grounded</li> <li>team feels protected</li> <li>stakeholders feel respected</li> </ul> <p>Everyone operates like professionals, not firefighters.</p>"},{"location":"209-faq/#10-how-3sf-supports-new-scope-decisions","title":"10. How 3SF Supports New Scope Decisions","text":""},{"location":"209-faq/#engagement-line-client-vendor","title":"Engagement Line (client \u2194 vendor)","text":"<p>The conversation becomes transparent and collaborative.</p>"},{"location":"209-faq/#delivery-line-vendor-product","title":"Delivery Line (vendor \u2194 product)","text":"<p>Teams stay stable because new scope is structured, not disruptive.</p>"},{"location":"209-faq/#value-line-product-client","title":"Value Line (product \u2194 client)","text":"<p>Every decision ties to value \u00d7 complexity, not internal pressure or politics.</p>"},{"location":"210-faq/","title":"How Do We Handle Contradictions Between Estimation Methods \u2014 FTEs, Story Points, Flow Metrics, and Forecasts?","text":"<p>FTE, story points, velocity, cycle time, and throughput appear to contradict one another because they measure different things. The Complexity &amp; Uncertainty Model acts as the unifying foundation that connects them into a single consistent system. Once complexity is scored and baselined, every estimation method aligns naturally \u2014 and contradictions become meaningful signals, not points of conflict.</p> <p>Why estimation methods appear to conflict, and how the Complexity &amp; Uncertainty Model aligns them into one coherent system.</p>"},{"location":"210-faq/#1-why-this-question-exists","title":"1. Why This Question Exists","text":"<p>Engineering organizations commonly use multiple estimation methods:</p> <ul> <li>FTE-based estimation for RFPs and budgeting</li> <li>Story Points for slicing and sprint planning</li> <li>Velocity for capacity signals</li> <li>Cycle time for flow predictability</li> <li>Throughput for forecasting</li> <li>PERT or range-based estimates for uncertainty</li> <li>Hours (occasionally, unfortunately) for effort visibility</li> <li>Discovery/design effort that often lives outside engineering metrics</li> </ul> <p>Teams quickly notice:</p> <ul> <li>FTE estimates don\u2019t align with story point estimates</li> <li>story points don\u2019t align with cycle time</li> <li>velocity goes up but delivery still slows down</li> <li>RFP numbers don\u2019t match actual complexity</li> <li>PM forecasts feel disconnected from engineering signals</li> <li>dependencies produce unpredictable effects</li> <li>\u201cparallel\u201d methods produce contradictory truths</li> </ul> <p>This page explains why these contradictions happen \u2014 and how the Complexity &amp; Uncertainty Model unifies everything.</p>"},{"location":"210-faq/#2-the-core-principle-these-methods-dont-match-because-they-measure-different-things","title":"2. The Core Principle: These Methods Don\u2019t Match Because They Measure Different Things","text":""},{"location":"210-faq/#fte-estimates-measure-capacity-needed","title":"FTE estimates measure capacity needed","text":""},{"location":"210-faq/#story-points-measure-relative-slicing-complexity","title":"Story Points measure relative slicing complexity","text":""},{"location":"210-faq/#velocity-measures-capacity-available","title":"Velocity measures capacity available","text":""},{"location":"210-faq/#cycle-time-measures-flow-efficiency","title":"Cycle time measures flow efficiency","text":""},{"location":"210-faq/#throughput-measures-actual-delivery-pace","title":"Throughput measures actual delivery pace","text":""},{"location":"210-faq/#discoverydesign-effort-measures-upstream-problem-complexity","title":"Discovery/design effort measures upstream problem complexity","text":"<p>These are not interchangeable signals.</p> <p>Trying to compare them directly is like comparing:</p> <ul> <li>temperature</li> <li>weight</li> <li>altitude</li> <li>wind speed</li> </ul> <p>Each has meaning \u2014 but not the same meaning.</p> <p>The contradictions appear because the organization has no shared complexity model tying them together.</p> <p>This model creates that missing alignment layer.</p>"},{"location":"210-faq/#3-how-the-model-aligns-all-estimation-methods-in-one-sentence","title":"3. How the Model Aligns All Estimation Methods (In One Sentence)","text":"<p>Complexity &amp; uncertainty scoring becomes the shared foundation that connects FTE estimates, story points, flow metrics, and forecasting into one system.</p> <p>Everything is derived from the same source, rather than invented in silos.</p> <p>Here\u2019s how.</p>"},{"location":"210-faq/#4-how-each-estimation-method-maps-to-complexity-uncertainty","title":"4. How Each Estimation Method Maps to Complexity &amp; Uncertainty","text":""},{"location":"210-faq/#1-fte-estimates-map-to-overall-complexity","title":"\u2b50 1. FTE Estimates \u2192 Map to Overall Complexity","text":"<p>FTE effort is calculated from:</p> <ul> <li>complexity score</li> <li>uncertainty modifier</li> <li>discovery overhead</li> <li>proportion of role effort</li> <li>multipliers based on risk</li> </ul> <p>This makes FTE-based estimation grounded, transparent, and traceable.</p>"},{"location":"210-faq/#2-story-points-map-to-local-complexity-within-a-feature","title":"\u2b50 2. Story Points \u2192 Map to Local Complexity Within a Feature","text":"<p>Story points are not free-floating guesses anymore.</p> <p>They are mapped to complexity ranges:</p> <ul> <li>Low complexity \u2192 1\u20133 SP</li> <li>Medium \u2192 5\u20138 SP</li> <li>High \u2192 13\u201321 SP</li> </ul> <p>This ties story points back to the RFP &amp; discovery assumptions.</p>"},{"location":"210-faq/#3-velocity-maps-to-capacity-not-scope","title":"\u2b50 3. Velocity \u2192 Maps to Capacity, Not Scope","text":"<p>Velocity is no longer compared to RFP expectations.</p> <p>Instead, it answers only one question:</p> <p>\u201cHow much work can this team predictably complete per sprint?\u201d</p> <p>Velocity becomes clean, honest, and uninflated because complexity stays external.</p>"},{"location":"210-faq/#4-cycle-time-maps-to-flow-efficiency","title":"\u2b50 4. Cycle Time \u2192 Maps to Flow Efficiency","text":"<p>Cycle time helps detect:</p> <ul> <li>bottlenecks</li> <li>flows blocked by external dependencies</li> <li>inconsistent slicing</li> <li>excessive WIP</li> <li>unstable refinement</li> </ul> <p>Cycle time is not compared to story points; it is compared to complexity drift.</p>"},{"location":"210-faq/#5-throughput-maps-to-predictable-delivery-pace","title":"\u2b50 5. Throughput \u2192 Maps to Predictable Delivery Pace","text":"<p>Throughput answers:</p> <p>\u201cHow many slices can we deliver over time?\u201d</p> <p>When slices are size-consistent (thanks to SP \u2192 complexity mapping), throughput becomes meaningful.</p>"},{"location":"210-faq/#6-uncertainty-maps-to-risk-forecasting-ranges","title":"\u2b50 6. Uncertainty \u2192 Maps to Risk &amp; Forecasting Ranges","text":"<p>High uncertainty \u2192 wide forecast ranges Low uncertainty \u2192 narrow ranges</p> <p>Forecasting becomes statistically meaningful instead of guesswork.</p>"},{"location":"210-faq/#7-discovery-design-effort-maps-to-uncertainty-burn-down","title":"\u2b50 7. Discovery &amp; Design Effort \u2192 Maps to Uncertainty Burn-Down","text":"<p>Discovery reduces uncertainty. Design reduces UX complexity.</p> <p>We no longer treat these as invisible activities. They directly influence baseline and drift.</p>"},{"location":"210-faq/#5-why-contradictions-disappear-after-alignment","title":"5. Why Contradictions Disappear After Alignment","text":""},{"location":"210-faq/#before-the-model","title":"Before the model:","text":"<ul> <li>FTE and SP estimates disagree</li> <li>velocity does not match RFP assumptions</li> <li>cycle time varies unpredictably</li> <li>throughput is inconsistent</li> <li>discovery effort invisible</li> <li>forecasting unrealistic</li> </ul>"},{"location":"210-faq/#after-the-model","title":"After the model:","text":"<ul> <li>FTE is derived from complexity</li> <li>SP is constrained by complexity</li> <li>velocity reflects actual capacity</li> <li>cycle time reflects flow quality</li> <li>throughput reflects slicing consistency</li> <li>discovery reduces uncertainty explicitly</li> <li>forecasting uses ranges tied to complexity</li> </ul> <p>Everything becomes one set of connected signals.</p>"},{"location":"210-faq/#6-when-contradictions-still-appear-and-what-they-mean","title":"6. When Contradictions Still Appear \u2014 And What They Mean","text":"<p>Contradictions are signals, not failures.</p>"},{"location":"210-faq/#example-contradictions-and-their-meaning","title":"Example contradictions and their meaning:","text":"<ul> <li> <p>SP increasing, but complexity baseline unchanged   \u2192 point inflation, incorrect mapping, or inconsistent slicing</p> </li> <li> <p>velocity increasing, but throughput decreasing   \u2192 re-estimation or slice size inflation</p> </li> <li> <p>FTE estimate stable, but drift increases   \u2192 missing upstream scoring or new scope revealed</p> </li> <li> <p>unpredictable cycle times   \u2192 dependency issues, unclear requirements, or high WIP</p> </li> <li> <p>forecasting ranges widening   \u2192 rising uncertainty, unclear discovery</p> </li> </ul> <p>Each contradiction tells us something specific about the system.</p> <p>The model teaches teams what that something is.</p>"},{"location":"210-faq/#7-how-pm-and-pl-use-alignment-in-real-conversations","title":"7. How PM and PL Use Alignment in Real Conversations","text":""},{"location":"210-faq/#pm-uses-alignment-to-explain","title":"PM uses alignment to explain:","text":"<ul> <li>value trade-offs</li> <li>timeline accuracy</li> <li>uncertainty evolution</li> <li>impact of new scope</li> <li>dependency risks</li> </ul>"},{"location":"210-faq/#pl-uses-alignment-to-explain","title":"PL uses alignment to explain:","text":"<ul> <li>technical drivers</li> <li>slicing feasibility</li> <li>delivery pacing</li> <li>drift sources</li> <li>alternative solutions</li> </ul> <p>Together, PM + PL unify narrative into a single, coherent truth.</p>"},{"location":"210-faq/#8-how-3sf-eliminates-conflicting-messages-to-clients","title":"8. How 3SF Eliminates Conflicting Messages to Clients","text":""},{"location":"210-faq/#engagement-line-client-vendor","title":"Engagement Line (client \u2194 vendor)","text":"<p>Client sees the real drivers behind estimates.</p>"},{"location":"210-faq/#delivery-line-vendor-product","title":"Delivery Line (vendor \u2194 product)","text":"<p>Teams use aligned signals; no contradictions.</p>"},{"location":"210-faq/#value-line-product-client","title":"Value Line (product \u2194 client)","text":"<p>Trade-offs become consistent and defensible.</p> <p>Contradictions are replaced with alignment across the triangular relationships.</p>"},{"location":"211-faq/","title":"When and How to Use Common Estimation Methods (T-Shirt, Story Points, PERT/Statistical PERT, etc.) \u2014 And How They Align Through the Model","text":"<p>Each estimation method serves a different purpose in the lifecycle \u2014 from coarse sizing (T-Shirt) to slicing (Story Points) to risk modeling (PERT) to flow prediction (Throughput &amp; Cycle Time). The Complexity &amp; Uncertainty Model aligns all of them by providing a shared foundation. This turns conflicting methods into a coherent, unified estimation system.</p> <p>A practical guide to the main estimation methods used in the industry, their purpose, and how the Complexity &amp; Uncertainty Framework connects them into a coherent system.</p>"},{"location":"211-faq/#1-why-this-page-exists","title":"1. Why This Page Exists","text":"<p>Every team uses estimation differently. Every coach teaches estimation differently. Every vendor uses different methods. Every PM has preferences. Every engineer has opinions.  </p> <p>This leads to confusion:</p> <ul> <li>\u201cWhen do we use T-Shirt sizes?\u201d</li> <li>\u201cDo we estimate Story Points or skip them?\u201d</li> <li>\u201cShould we use PERT?\u201d</li> <li>\u201cWhat about range-based estimation?\u201d</li> <li>\u201cWhat method is \u2018best\u2019?\u201d</li> <li>\u201cWhy do we have multiple methods at all?\u201d</li> </ul> <p>This page explains:</p> <ul> <li>the purpose of each method</li> <li>when to use it</li> <li>what it measures</li> <li>how to align it with complexity &amp; uncertainty scoring</li> <li>how each method fits into the lifecycle</li> </ul> <p>This gives teams a shared truth.</p>"},{"location":"211-faq/#2-summary-table-quick-reference","title":"2. Summary Table (Quick Reference)","text":"Method Purpose Used In What It Measures Resolution How It Aligns T-Shirt Sizes (XS\u2013XL) Fast relative sizing RFP, early discovery Rough complexity Very low Maps to complexity drivers Story Points Slice complexity for delivery Refinement, planning Implementation complexity Medium Mapped to complexity ranges PERT Single-feature effort range RFP/Discovery Best/likely/worst Medium-high Informed by complexity + uncertainty Statistical PERT Probabilistic forecasting RFP, planning Confidence ranges High Uses complexity baseline + uncertainty Cycle Time Flow predictability Delivery Work duration High accuracy Validates slicing &amp; drift Throughput Real delivery pace Delivery Units per time High Anchored by consistent slice size Hours (discouraged) Time spent Exceptions only Actual effort High Not aligned (effort \u2260 complexity) <p>This is your \u201ccheat sheet.\u201d</p>"},{"location":"211-faq/#3-t-shirt-sizes-xsxl","title":"3. T-Shirt Sizes (XS\u2013XL)","text":""},{"location":"211-faq/#purpose","title":"Purpose:","text":"<p>Fast, low-effort sizing of features or EPICs.</p>"},{"location":"211-faq/#best-for","title":"Best For:","text":"<ul> <li>RFP</li> <li>early discovery</li> <li>mass-estimating many features</li> <li>aligning cross-functional understanding</li> </ul>"},{"location":"211-faq/#why-it-exists","title":"Why It Exists:","text":"<p>Teams need a way to quickly assess relative complexity without precision.</p>"},{"location":"211-faq/#key-limitation","title":"Key Limitation:","text":"<p>T-shirts tell us nothing about effort or timeline.</p>"},{"location":"211-faq/#how-it-aligns-with-the-model","title":"How It Aligns With the Model:","text":"<p>T-Shirts are an informal proxy for the complexity drivers.</p> <p>Example mapping:</p> T-Shirt Complexity Score XS 1\u20133 S 4\u20136 M 7\u201310 L 11\u201315 XL 16+ <p>This ensures T-shirts \u2192 complexity \u2192 FTE \u2192 SP \u2192 forecasting all connect.</p>"},{"location":"211-faq/#when-to-use-it","title":"When to Use It:","text":"<ul> <li>Kickoffs</li> <li>Discovery workshops</li> <li>RFP scope assessment</li> <li>First-pass sequencing</li> </ul>"},{"location":"211-faq/#4-story-points","title":"4. Story Points","text":""},{"location":"211-faq/#purpose_1","title":"Purpose:","text":"<p>Relative complexity of implementation slices (stories).</p>"},{"location":"211-faq/#used-in","title":"Used In:","text":"<ul> <li>refinement</li> <li>sprint planning</li> <li>flow forecasting</li> </ul>"},{"location":"211-faq/#what-they-measure","title":"What They Measure:","text":"<ul> <li>how complex it is to deliver a slice</li> <li>not scope</li> <li>not effort</li> <li>not time</li> </ul>"},{"location":"211-faq/#why-story-points-fail","title":"Why Story Points Fail:","text":"<p>They are asked to do what they cannot do \u2014 reflect scope, risk, or cost.</p>"},{"location":"211-faq/#how-story-points-align-with-the-model","title":"How Story Points Align With the Model:","text":"<p>SP map to complexity ranges:</p> Complexity Level SP Range Low 1\u20133 Medium 5\u20138 High 13\u201321 Extreme split <p>This prevents inflation and aligns SP with RFP assumptions.</p>"},{"location":"211-faq/#when-to-use-sp","title":"When to Use SP:","text":"<ul> <li>after Discovery</li> <li>when slicing features into increments</li> <li>for delivery forecasting (velocity / throughput)</li> </ul>"},{"location":"211-faq/#5-pert-optimistic-most-likely-pessimistic","title":"5. PERT (Optimistic, Most Likely, Pessimistic)","text":""},{"location":"211-faq/#purpose_2","title":"Purpose:","text":"<p>Estimates effort range for a single feature or integration.</p>"},{"location":"211-faq/#used-in_1","title":"Used In:","text":"<ul> <li>RFP</li> <li>discovery</li> <li>architect-level estimation</li> <li>risky integrations</li> <li>NFR-heavy features</li> </ul>"},{"location":"211-faq/#formula","title":"Formula:","text":"<p>(O + 4M + P) / 6</p>"},{"location":"211-faq/#why-its-useful","title":"Why It\u2019s Useful:","text":"<p>It gives a range, not a false single-point estimate.</p>"},{"location":"211-faq/#how-it-aligns-with-the-model_1","title":"How It Aligns With the Model:","text":"<ul> <li>PERT \u201cOptimistic\u201d = low uncertainty + low complexity</li> <li>PERT \u201cLikely\u201d = complexity baseline</li> <li>PERT \u201cPessimistic\u201d = high uncertainty + drift risk</li> </ul>"},{"location":"211-faq/#when-to-use-pert","title":"When to Use PERT:","text":"<ul> <li>high-risk features</li> <li>unpredictable integrations</li> <li>third-party services</li> <li>absence of clear requirements</li> <li>infrastructure work</li> </ul>"},{"location":"211-faq/#6-statistical-pert","title":"6. Statistical PERT","text":""},{"location":"211-faq/#purpose_3","title":"Purpose:","text":"<p>A more accurate probabilistic forecast using confidence curves.</p>"},{"location":"211-faq/#used-in_2","title":"Used In:","text":"<ul> <li>RFP timeline ranges</li> <li>Program-level forecasting</li> <li>Multi-team forecasting</li> <li>Portfolio planning</li> </ul>"},{"location":"211-faq/#why-its-powerful","title":"Why It\u2019s Powerful:","text":"<ul> <li>adds probability to pessimistic estimates</li> <li>shows confidence intervals</li> <li>helps explain risk to stakeholders</li> <li>captures uncertainty more accurately</li> </ul>"},{"location":"211-faq/#how-it-aligns-with-the-model_2","title":"How It Aligns With the Model:","text":"<p>Statistical PERT uses:</p> <ul> <li>complexity baseline as mode</li> <li>uncertainty score as pessimistic width</li> <li>drift history to adjust risk curve</li> </ul> <p>It is the most precise method for major RFPs.</p>"},{"location":"211-faq/#when-to-use-it_1","title":"When to Use It:","text":"<ul> <li>enterprise programs</li> <li>multi-vendor implementations</li> <li>architecture-heavy domains</li> <li>when the business needs ranges, not guesses</li> </ul>"},{"location":"211-faq/#7-cycle-time","title":"7. Cycle Time","text":""},{"location":"211-faq/#purpose_4","title":"Purpose:","text":"<p>Measure how long work actually takes once started.</p>"},{"location":"211-faq/#used-in_3","title":"Used In:","text":"<ul> <li>forecasting</li> <li>bottleneck detection</li> <li>flow optimization</li> <li>evaluating process health</li> </ul>"},{"location":"211-faq/#why-its-important","title":"Why It\u2019s Important:","text":"<p>Cycle time is the most accurate operational metric in Agile delivery.</p>"},{"location":"211-faq/#how-it-aligns-with-the-model_3","title":"How It Aligns With the Model:","text":"<p>Cycle time should correlate with:</p> <ul> <li>complexity ranges</li> <li>slice consistency</li> <li>drift detection</li> </ul> <p>If cycle time spikes \u2192 complexity or uncertainty are higher than expected.</p>"},{"location":"211-faq/#when-to-use-it_2","title":"When to Use It:","text":"<ul> <li>after 2\u20134 sprints</li> <li>once slice sizes stabilize</li> </ul>"},{"location":"211-faq/#8-throughput","title":"8. Throughput","text":""},{"location":"211-faq/#purpose_5","title":"Purpose:","text":"<p>Measure number of \u201cdone\u201d items per time period.</p>"},{"location":"211-faq/#used-in_4","title":"Used In:","text":"<ul> <li>forecasting</li> <li>capacity planning</li> <li>risk planning</li> </ul>"},{"location":"211-faq/#how-it-aligns","title":"How It Aligns:","text":"<p>Throughput becomes meaningful only if slices (SP) are consistent \u2014 which the model ensures.</p>"},{"location":"211-faq/#9-hours-actual-effort","title":"9. Hours (Actual Effort)","text":""},{"location":"211-faq/#purpose_6","title":"Purpose:","text":"<p>Track actual effort for:</p> <ul> <li>billing</li> <li>compliance</li> <li>certain enterprise environments</li> </ul>"},{"location":"211-faq/#why-hours-are-not-good-for-estimation","title":"Why Hours Are Not Good for Estimation:","text":"<p>Hours measure effort, not complexity.</p>"},{"location":"211-faq/#how-to-align-them","title":"How to Align Them:","text":"<p>Hours should be used only after delivery, not as an estimation input.</p>"},{"location":"211-faq/#10-choosing-the-right-method-at-the-right-time","title":"10. Choosing the Right Method at the Right Time","text":"<p>Here is the recommended sequence across the project lifecycle.</p>"},{"location":"211-faq/#rfp-stage-unknowns-high","title":"\u2b50 RFP Stage (Unknowns High)","text":"<ul> <li>T-Shirt sizes</li> <li>Complexity scoring</li> <li>Uncertainty scoring</li> <li>PERT / Statistical PERT</li> <li>FTE calculation</li> </ul> <p>Output: Cost range + timeline range.</p>"},{"location":"211-faq/#discovery-stage-unknowns-decreasing","title":"\u2b50 Discovery Stage (Unknowns Decreasing)","text":"<ul> <li>Updated complexity scoring</li> <li>Updated uncertainty scoring</li> <li>PERT refinement</li> <li>UX flow estimation</li> <li>Architecture alignment</li> </ul> <p>Output: Baseline v2 (more accurate complexity model).</p>"},{"location":"211-faq/#delivery-stage-work-in-motion","title":"\u2b50 Delivery Stage (Work in Motion)","text":"<ul> <li>Story Points (mapped to complexity)</li> <li>Throughput</li> <li>Cycle time</li> <li>Drift tracking</li> <li>Uncertainty burn-down</li> </ul> <p>Output: Predictable flow + early detection of drift.</p>"},{"location":"211-faq/#forecasting-planning-resets","title":"\u2b50 Forecasting &amp; Planning Resets","text":"<ul> <li>Statistical PERT</li> <li>Throughput-based forecasting</li> <li>Complexity drift analysis</li> <li>Value \u00d7 Complexity sequencing</li> </ul> <p>Output: Updated timelines, trade-offs, and stakeholder alignment.</p>"},{"location":"211-faq/#11-how-the-model-connects-all-estimation-methods-into-one-system","title":"11. How the Model Connects All Estimation Methods into One System","text":"<p>Without the model, estimation methods contradict each other. With the model:</p> <ul> <li>T-Shirt \u2192 complexity approximate</li> <li>Complexity \u2192 SP ranges</li> <li>SP \u2192 throughput &amp; forecasting</li> <li>Complexity \u00d7 uncertainty \u2192 PERT</li> <li>PERT \u2192 timeline ranges</li> <li>Cycle time \u2194 drift detection</li> <li>Throughput \u2194 capacity predictability</li> </ul> <p>Everything connects.</p> <p>This is the missing \u201ctranslation layer\u201d the industry never had.</p>"}]}