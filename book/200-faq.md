# **Quick FAQ Review**

* [How This Model Fits With Existing RFP and Fixed-Bid Estimation Processes?](201-faq.md)  
    > **Discovery estimates complexity & uncertainty, Delivery estimates slices & story points, and PM/PL keep both tracks aligned by maintaining a shared Complexity Baseline, a readiness buffer, and weekly track syncs.**  
    > This makes Dual-Track predictable, measurable, and aligned end-to-end.
* [How Do We Prevent Story Point Inflation or Re-Estimation Games?](202-faq.md)  
    > **We prevent story point inflation by anchoring SP to complexity ranges, scoring complexity separately, updating the baseline when drift occurs, and using SP only for slicing and flow — never for forecasting, cost, or performance.  
    > This restores honesty, stability, and predictability.**
* [Who Is Responsible for Scoring Complexity & Uncertainty — and How Often Do We Update the Baseline?](203-faq.md)  
    > **Complexity & uncertainty scoring is a multi-role activity:  
    > PM, BA, PD, Architect, and EM each score the parts they understand best.  
    > The baseline is created during RFP (v1), updated after Discovery (v2), and only updated during Delivery when real complexity drift occurs (v3).  
    > Drift tracking is continuous — baseline updates are intentional.**
* [How Do We Handle Complexity Drift Without Triggering Client Escalations or Immediate Renegotiation?](204-faq.md)  
    > **We handle complexity drift through early detection, internal alignment, structured client conversations, and fact-based trade-offs — not by triggering immediate renegotiation.  
    > Complexity drift is normal; surprises are not.**
* [How Does This Model Scale in Multi-Team or Multi-Vendor Environments?](205-faq.md)  
    > **The model scales by giving all teams and vendors a shared set of complexity & uncertainty drivers, a unified baseline, a shared dependency map, and a program-level drift dashboard.  
    > Each team estimates independently, but the system aligns at the program level — creating predictable multi-team delivery.**
* [How Estimation Aligns Across Dual-Track Scrum (Discovery + Delivery)?](206-faq.md)  
    > **Discovery estimates complexity & uncertainty, Delivery estimates slices & story points, and PM/PL keep both tracks aligned by maintaining a shared Complexity Baseline, a readiness buffer, and weekly track syncs.**  
    > This makes Dual-Track predictable, measurable, and aligned end-to-end.
* [Does Tracking Complexity and Uncertainty Increase Overhead? How Do We Keep It Lightweight?](207-faq.md)  
    > **No — tracking complexity and uncertainty does not add overhead.  
    > It replaces invisible, high-cost overhead (rework, re-estimation, surprises) with a fast, lightweight alignment tool.  
    > Teams score only meaningful features, only when needed, using a simple scale — and save significant effort later in delivery.**
* [How Does This Model Help Us Make Scope, Sequencing, and Value-Tradeoff Decisions With Clients?](208-faq.md)  
    > **The model enables clear, evidence-based decisions about scope, sequencing, and value by making complexity, uncertainty, and drift visible.  
    > It turns trade-off discussions into structured, data-driven conversations rather than subjective debates — and helps clients make informed decisions.**
* [What Happens When New Scope Appears Mid-Project — How Do We Score, Track, and Communicate It?](209-faq.md)  
    > **When new scope appears, we score it using the same complexity and uncertainty drivers, compare it against the baseline, and present clear trade-off options to the client.  
    > This keeps delivery stable, protects the team, and turns scope changes into professional, data-driven decisions — not disruption.**
* [How Do We Handle Contradictions Between Estimation Methods — FTEs, Story Points, Flow Metrics, and Forecasts?](210-faq.md)  
    > **FTE, story points, velocity, cycle time, and throughput appear to contradict one another because they measure different things.  
    > The Complexity & Uncertainty Model acts as the unifying foundation that connects them into a single consistent system.  
    > Once complexity is scored and baselined, every estimation method aligns naturally — and contradictions become meaningful signals, not points of conflict.**
* [When and How to Use Common Estimation Methods (T-Shirt, Story Points, PERT/Statistical PERT, etc.) — And How They Align Through the Model?](211-faq.md)  
    > **Each estimation method serves a different purpose in the lifecycle — from coarse sizing (T-Shirt) to slicing (Story Points) to risk modeling (PERT) to flow prediction (Throughput & Cycle Time).  
    > The Complexity & Uncertainty Model aligns all of them by providing a shared foundation.  
    > This turns conflicting methods into a coherent, unified estimation system.**
