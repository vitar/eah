# **Who Is Responsible for Scoring Complexity & Uncertainty — and How Often Do We Update the Baseline?**

> **Complexity & uncertainty scoring is a multi-role activity:  
> PM, BA, PD, Architect, and EM each score the parts they understand best.  
> The baseline is created during RFP (v1), updated after Discovery (v2), and only updated during Delivery when real complexity drift occurs (v3).  
> Drift tracking is continuous — baseline updates are intentional.**

---

*Clarifying ownership, timing, and collaboration across roles.*

---

## **1. Why This Question Exists**

Every organization hits the same three pain points:

1. **No one is sure who should score what**
   PM? BA? PD? Architect? Devs? EM? PL? All? None?

2. **Scoring happens at the wrong times**
   Too early → inaccurate
   Too late → chaotic
   Repeatedly → wasteful
   Never → dangerous

3. **Baselines are created once and then forgotten**
   Delivery inherits a “fixed estimate” with no way to evaluate how assumptions changed.

This page explains exactly **who owns scoring**, **who contributes**, and **how frequently** the baseline should be updated.

---

## **2. Core Principle: Scoring Is Cross-Functional, Ownership Is Shared**

Complexity & uncertainty *cannot* be scored by a single role, because each role has visibility into different dimensions:

* BA → requirement clarity
* PD → UX & workflow complexity
* Architect → integration, NFR, data, domain
* PM → stakeholder alignment, dependency risk
* EM/Lead → implementation complexity
* Devs → slicing complexity, technical nuance

**This is fundamentally a multi-role activity.**

But ownership must be clear.

---

## **3. Ownership Model (RACI)**

| Activity                            | PM  | BA | PD | Architect | EM/PL | Dev Team |
| ----------------------------------- | --- | -- | -- | --------- | ----- | -------- |
| Identify EPIC/Feature list          | A/R | C  | C  | C         | C     | —        |
| Score Complexity                    | A   | R  | R  | R         | R     | C        |
| Score Uncertainty                   | A   | R  | R  | R         | C     | —        |
| Create Baseline v1 (RFP)            | A/R | C  | C  | R         | C     | —        |
| Create Baseline v2 (Post-Discovery) | A/R | R  | R  | R         | C     | C        |
| Track Drift                         | A/R | C  | C  | C         | A/R   | C        |
| Trigger Baseline Update             | A   | —  | —  | R         | R     | —        |

Legend:
**A = Accountable**, **R = Responsible**, **C = Consulted**

This matrix keeps responsibility distributed without creating ambiguity.

---

## **4. What Each Role Scores (Breakdown)**

### **PM (Project/Program Manager)**

Scores:

* requirement clarity
* stakeholder alignment
* delivery constraints
* external dependencies
* operational/process uncertainty

Why: PMs manage alignment and risk visibility.

---

### **BA (Business Analyst)**

Scores:

* requirement depth
* ambiguous logic
* gaps in acceptance criteria
* documentation quality

Why: BAs understand functional truth and its vagueness.

---

### **PD (Product Designer / UX)**

Scores:

* UI/UX complexity
* workflow branching
* user interactions
* design uncertainty

Why: UX complexity is often the hidden multiplier in delivery effort.

---

### **Architect**

Scores:

* integration complexity
* data complexity
* non-functional requirements
* platform constraints
* technical unknowns

Why: architecture drives the upper bound of engineering effort.

---

### **EM/PL (Engineering Manager / Project Lead)**

Scores:

* implementation complexity
* technical dependencies
* slicing feasibility
* testability complexity

Why: EM/PLs own the delivery execution.

---

### **Developers**

Contribute to:

* technical nuance
* incremental complexity
* edge-case awareness
* splitting stories

Why: devs are closest to the code and can detect hidden complexity early.

---

## **5. When To Score (Frequency)**

The baseline is not static.
It evolves as clarity evolves.

Below is the recommended update rhythm.

---

### ⭐ **Baseline v1 — RFP Stage (Initial Assumptions)**

When:

* first estimate
* early capability breakdown

Who:

* PM + Architect + BA + PD + EM

Purpose:

* guide costing
* estimate timeline
* inform staffing
* establish assumptions

---

### ⭐ **Baseline v2 — After Discovery & Design (Clarity Pass)**

When:

* after requirements clarified
* after UX flows drafted
* after architecture is mapped
* after key dependencies identified

Who:

* PM + BA + PD + Architect

Purpose:

* convert assumptions → validated complexity
* reduce uncertainty
* enable stable sprint planning
* adjust forecasts before coding starts

This is the **most important** baseline version.

---

### ⭐ **Baseline v3 — During Delivery (Drift Detection)**

When:

* complexity drift crosses threshold (e.g., 15–25%)
* major new scope appears
* new integration constraints discovered
* dependencies change
* uncertainty spikes

Who:

* PM + Architect + EM/PL

Purpose:

* stay ahead of drift
* adjust timeline transparently
* prevent velocity pressure
* justify re-estimation
* protect team and client expectations

Baseline v3 is **not automatic** — it’s triggered by real events.

---

### ⭐ **Ongoing Drift Tracking — Every Sprint**

Frequency:

* Complexity rescored when new info arrives
* Uncertainty reviewed weekly
* Drift trend reviewed in sprint reviews or PM/PL sync

Purpose:

* detect misalignment early
* prevent “slow deterioration”
* avoid late surprises
* create predictable flow

---

## **6. When NOT To Update the Baseline**

The baseline should *not* be updated:

* to “fix” velocity
* to make SP look right
* to hide complexity
* to force artificial alignment
* because someone feels uncomfortable
* because leadership wants a smoother forecast

If none of the *drivers* changed — the baseline must remain stable.

This maintains the integrity of drift signals.

---

## **7. How This Prevents Estimation Chaos**

This model ensures:

### **No single role is overburdened**

Complexity is scored by experts in each domain.

### **Delivery inherits a realistic context**

Baseline makes assumptions visible.

### **Uncertainty decreases predictably**

Burn-down reveals discovery gaps.

### **Drift becomes measurable**

Teams no longer rely on “it feels harder.”

### **Re-estimation becomes justified**

Because drift is visible, not emotional.

### **Velocity becomes clean**

No point inflation, no gaming.

### **Stakeholder conversations improve**

Discussion becomes factual, not political.
