# ⭐ **When and How to Use Common Estimation Methods (T-Shirt, Story Points, PERT/Statistical PERT, etc.) — And How They Align Through the Model**

> **Each estimation method serves a different purpose in the lifecycle — from coarse sizing (T-Shirt) to slicing (Story Points) to risk modeling (PERT) to flow prediction (Throughput & Cycle Time).  
> The Complexity & Uncertainty Model aligns all of them by providing a shared foundation.  
> This turns conflicting methods into a coherent, unified estimation system.**

---

*A practical guide to the main estimation methods used in the industry, their purpose, and how the Complexity & Uncertainty Framework connects them into a coherent system.*

---

## **1. Why This Page Exists**

Every team uses estimation differently.  
Every coach teaches estimation differently.  
Every vendor uses different methods.  
Every PM has preferences.  
Every engineer has opinions.  

This leads to confusion:

* “When do we use T-Shirt sizes?”
* “Do we estimate Story Points or skip them?”
* “Should we use PERT?”
* “What about range-based estimation?”
* “What method is ‘best’?”
* “Why do we have multiple methods at all?”

This page explains:

* **the purpose of each method**
* **when to use it**
* **what it measures**
* **how to align it with complexity & uncertainty scoring**
* **how each method fits into the lifecycle**

This gives teams a shared truth.

---

## **2. Summary Table (Quick Reference)**

| Method                    | Purpose                       | Used In              | What It Measures          | Resolution    | How It Aligns                          |
| ------------------------- | ----------------------------- | -------------------- | ------------------------- | ------------- | -------------------------------------- |
| **T-Shirt Sizes (XS–XL)** | Fast relative sizing          | RFP, early discovery | Rough complexity          | Very low      | Maps to complexity drivers             |
| **Story Points**          | Slice complexity for delivery | Refinement, planning | Implementation complexity | Medium        | Mapped to complexity ranges            |
| **PERT**                  | Single-feature effort range   | RFP/Discovery        | Best/likely/worst         | Medium-high   | Informed by complexity + uncertainty   |
| **Statistical PERT**      | Probabilistic forecasting     | RFP, planning        | Confidence ranges         | High          | Uses complexity baseline + uncertainty |
| **Cycle Time**            | Flow predictability           | Delivery             | Work duration             | High accuracy | Validates slicing & drift              |
| **Throughput**            | Real delivery pace            | Delivery             | Units per time            | High          | Anchored by consistent slice size      |
| **Hours** (discouraged)   | Time spent                    | Exceptions only      | Actual effort             | High          | Not aligned (effort ≠ complexity)      |

This is your “cheat sheet.”

---

## **3. T-Shirt Sizes (XS–XL)**

### **Purpose:**

Fast, low-effort sizing of *features* or *EPICs*.

### **Best For:**

* RFP
* early discovery
* mass-estimating many features
* aligning cross-functional understanding

### **Why It Exists:**

Teams need a way to quickly assess relative complexity without precision.

### **Key Limitation:**

T-shirts tell us nothing about effort or timeline.

### **How It Aligns With the Model:**

T-Shirts are an **informal proxy** for the complexity drivers.

Example mapping:

| T-Shirt | Complexity Score |
| ------- | ---------------- |
| XS      | 1–3              |
| S       | 4–6              |
| M       | 7–10             |
| L       | 11–15            |
| XL      | 16+              |

This ensures T-shirts → complexity → FTE → SP → forecasting all connect.

### **When to Use It:**

* Kickoffs
* Discovery workshops
* RFP scope assessment
* First-pass sequencing

---

## **4. Story Points**

### **Purpose:**

Relative complexity of *implementation slices* (stories).

### **Used In:**

* refinement
* sprint planning
* flow forecasting

### **What They Measure:**

* how complex it is to deliver a slice
* *not* scope
* *not* effort
* *not* time

### **Why Story Points Fail:**

They are asked to do what they cannot do — reflect scope, risk, or cost.

### **How Story Points Align With the Model:**

SP map to complexity ranges:

| Complexity Level | SP Range |
| ---------------- | -------- |
| Low              | 1–3      |
| Medium           | 5–8      |
| High             | 13–21    |
| Extreme          | split    |

This prevents inflation and aligns SP with RFP assumptions.

### **When to Use SP:**

* after Discovery
* when slicing features into increments
* for delivery forecasting (velocity / throughput)

---

## **5. PERT (Optimistic, Most Likely, Pessimistic)**

### **Purpose:**

Estimates effort range for a *single feature* or *integration*.

### **Used In:**

* RFP
* discovery
* architect-level estimation
* risky integrations
* NFR-heavy features

### **Formula:**

*(O + 4M + P) / 6*

### **Why It’s Useful:**

It gives a **range**, not a false single-point estimate.

### **How It Aligns With the Model:**

* PERT “Optimistic” = low uncertainty + low complexity
* PERT “Likely” = complexity baseline
* PERT “Pessimistic” = high uncertainty + drift risk

### **When to Use PERT:**

* high-risk features
* unpredictable integrations
* third-party services
* absence of clear requirements
* infrastructure work

---

## **6. Statistical PERT**

### **Purpose:**

A more accurate probabilistic forecast using confidence curves.

### **Used In:**

* RFP timeline ranges
* Program-level forecasting
* Multi-team forecasting
* Portfolio planning

### **Why It’s Powerful:**

* adds probability to pessimistic estimates
* shows confidence intervals
* helps explain risk to stakeholders
* captures uncertainty more accurately

### **How It Aligns With the Model:**

Statistical PERT uses:

* **complexity baseline** as mode
* **uncertainty score** as pessimistic width
* drift history to adjust risk curve

It is the most precise method for major RFPs.

### **When to Use It:**

* enterprise programs
* multi-vendor implementations
* architecture-heavy domains
* when the business needs ranges, not guesses

---

## **7. Cycle Time**

### **Purpose:**

Measure how long work *actually* takes once started.

### **Used In:**

* forecasting
* bottleneck detection
* flow optimization
* evaluating process health

### **Why It’s Important:**

Cycle time is the **most accurate operational metric** in Agile delivery.

### **How It Aligns With the Model:**

Cycle time should correlate with:

* complexity ranges
* slice consistency
* drift detection

If cycle time spikes → complexity or uncertainty are higher than expected.

### **When to Use It:**

* after 2–4 sprints
* once slice sizes stabilize

---

## **8. Throughput**

### **Purpose:**

Measure number of “done” items per time period.

### **Used In:**

* forecasting
* capacity planning
* risk planning

### **How It Aligns:**

Throughput becomes meaningful only if slices (SP) are consistent — which the model ensures.

---

## **9. Hours (Actual Effort)**

### **Purpose:**

Track actual effort for:

* billing
* compliance
* certain enterprise environments

### **Why Hours Are Not Good for Estimation:**

Hours measure *effort*, not *complexity*.

### **How to Align Them:**

Hours should be used **only after delivery**, not as an estimation input.

---

## **10. Choosing the Right Method at the Right Time**

Here is the recommended sequence across the project lifecycle.

---

### ⭐ **RFP Stage (Unknowns High)**

* T-Shirt sizes
* Complexity scoring
* Uncertainty scoring
* PERT / Statistical PERT
* FTE calculation

**Output:**  
Cost range + timeline range.

---

### ⭐ **Discovery Stage (Unknowns Decreasing)**

* Updated complexity scoring
* Updated uncertainty scoring
* PERT refinement
* UX flow estimation
* Architecture alignment

**Output:**  
Baseline v2 (more accurate complexity model).

---

### ⭐ **Delivery Stage (Work in Motion)**

* Story Points (mapped to complexity)
* Throughput
* Cycle time
* Drift tracking
* Uncertainty burn-down

**Output:**  
Predictable flow + early detection of drift.

---

### ⭐ **Forecasting & Planning Resets**

* Statistical PERT
* Throughput-based forecasting
* Complexity drift analysis
* Value × Complexity sequencing

**Output:**  
Updated timelines, trade-offs, and stakeholder alignment.

---

## **11. How the Model Connects All Estimation Methods into One System**

Without the model, estimation methods contradict each other.  
With the model:

* T-Shirt → complexity approximate
* Complexity → SP ranges
* SP → throughput & forecasting
* Complexity × uncertainty → PERT
* PERT → timeline ranges
* Cycle time ↔ drift detection
* Throughput ↔ capacity predictability

Everything connects.

**This is the missing “translation layer” the industry never had.**
