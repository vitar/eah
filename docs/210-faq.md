# ⭐ **How Do We Handle Contradictions Between Estimation Methods — FTEs, Story Points, Flow Metrics, and Forecasts?**

> **FTE, story points, velocity, cycle time, and throughput appear to contradict one another because they measure different things.  
> The Complexity & Uncertainty Model acts as the unifying foundation that connects them into a single consistent system.  
> Once complexity is scored and baselined, every estimation method aligns naturally — and contradictions become meaningful signals, not points of conflict.**

---

*Why estimation methods appear to conflict, and how the Complexity & Uncertainty Model aligns them into one coherent system.*

---

## **1. Why This Question Exists**

Engineering organizations commonly use multiple estimation methods:

* **FTE-based estimation** for RFPs and budgeting
* **Story Points** for slicing and sprint planning
* **Velocity** for capacity signals
* **Cycle time** for flow predictability
* **Throughput** for forecasting
* **PERT or range-based estimates** for uncertainty
* **Hours** (occasionally, unfortunately) for effort visibility
* **Discovery/design effort** that often lives outside engineering metrics

Teams quickly notice:

* FTE estimates don’t align with story point estimates
* story points don’t align with cycle time
* velocity goes up but delivery still slows down
* RFP numbers don’t match actual complexity
* PM forecasts feel disconnected from engineering signals
* dependencies produce unpredictable effects
* “parallel” methods produce *contradictory truths*

This page explains *why* these contradictions happen — and how the Complexity & Uncertainty Model unifies everything.

---

## **2. The Core Principle: These Methods Don’t Match Because They Measure Different Things**

### **FTE estimates** measure *capacity needed*

### **Story Points** measure *relative slicing complexity*

### **Velocity** measures *capacity available*

### **Cycle time** measures *flow efficiency*

### **Throughput** measures *actual delivery pace*

### **Discovery/design effort** measures *upstream problem complexity*

These are not interchangeable signals.

Trying to compare them directly is like comparing:

* temperature
* weight
* altitude
* wind speed

Each has meaning — but not the same meaning.

The contradictions appear because the *organization has no shared complexity model tying them together.*

This model creates that missing alignment layer.

---

## **3. How the Model Aligns All Estimation Methods (In One Sentence)**

> **Complexity & uncertainty scoring becomes the shared foundation that connects FTE estimates, story points, flow metrics, and forecasting into one system.**

Everything is derived from the same source, rather than invented in silos.

Here’s how.

---

## **4. How Each Estimation Method Maps to Complexity & Uncertainty**

### ⭐ **1. FTE Estimates → Map to Overall Complexity**

FTE effort is calculated from:

* complexity score
* uncertainty modifier
* discovery overhead
* proportion of role effort
* multipliers based on risk

This makes FTE-based estimation grounded, transparent, and traceable.

---

### ⭐ **2. Story Points → Map to Local Complexity Within a Feature**

Story points are not free-floating guesses anymore.

They are mapped to complexity ranges:

* Low complexity → 1–3 SP
* Medium → 5–8 SP
* High → 13–21 SP

This ties story points back to the RFP & discovery assumptions.

---

### ⭐ **3. Velocity → Maps to Capacity, Not Scope**

Velocity is no longer compared to RFP expectations.

Instead, it answers only one question:

> “How much work can this team *predictably* complete per sprint?”

Velocity becomes clean, honest, and uninflated because complexity stays external.

---

### ⭐ **4. Cycle Time → Maps to Flow Efficiency**

Cycle time helps detect:

* bottlenecks
* flows blocked by external dependencies
* inconsistent slicing
* excessive WIP
* unstable refinement

Cycle time is not compared to story points; it is compared to complexity drift.

---

### ⭐ **5. Throughput → Maps to Predictable Delivery Pace**

Throughput answers:

> “How many slices can we deliver over time?”

When slices are size-consistent (thanks to SP → complexity mapping), throughput becomes meaningful.

---

### ⭐ **6. Uncertainty → Maps to Risk & Forecasting Ranges**

High uncertainty → wide forecast ranges
Low uncertainty → narrow ranges

Forecasting becomes statistically meaningful instead of guesswork.

---

### ⭐ **7. Discovery & Design Effort → Maps to Uncertainty Burn-Down**

Discovery reduces uncertainty.
Design reduces UX complexity.

We no longer treat these as invisible activities.
They directly influence baseline and drift.

---

## **5. Why Contradictions Disappear After Alignment**

### **Before the model:**

* FTE and SP estimates disagree
* velocity does not match RFP assumptions
* cycle time varies unpredictably
* throughput is inconsistent
* discovery effort invisible
* forecasting unrealistic

### **After the model:**

* FTE is derived from complexity
* SP is constrained by complexity
* velocity reflects actual capacity
* cycle time reflects flow quality
* throughput reflects slicing consistency
* discovery reduces uncertainty explicitly
* forecasting uses ranges tied to complexity

Everything becomes **one set of connected signals.**

---

## **6. When Contradictions Still Appear — And What They Mean**

Contradictions are *signals*, not failures.

### Example contradictions and their meaning:

* **SP increasing, but complexity baseline unchanged**
  → point inflation, incorrect mapping, or inconsistent slicing

* **velocity increasing, but throughput decreasing**
  → re-estimation or slice size inflation

* **FTE estimate stable, but drift increases**
  → missing upstream scoring or new scope revealed

* **unpredictable cycle times**
  → dependency issues, unclear requirements, or high WIP

* **forecasting ranges widening**
  → rising uncertainty, unclear discovery

Each contradiction tells us something specific about the system.

The model teaches teams *what that something is*.

---

## **7. How PM and PL Use Alignment in Real Conversations**

### PM uses alignment to explain:

* value trade-offs
* timeline accuracy
* uncertainty evolution
* impact of new scope
* dependency risks

### PL uses alignment to explain:

* technical drivers
* slicing feasibility
* delivery pacing
* drift sources
* alternative solutions

Together, PM + PL unify narrative into a single, coherent truth.

---

## **8. How 3SF Eliminates Conflicting Messages to Clients**

### **Engagement Line (client ↔ vendor)**

Client sees the real drivers behind estimates.

### **Delivery Line (vendor ↔ product)**

Teams use aligned signals; no contradictions.

### **Value Line (product ↔ client)**

Trade-offs become consistent and defensible.

Contradictions are replaced with alignment across the triangular relationships.
